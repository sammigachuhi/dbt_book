[["index.html", "dbt Book Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " dbt Book Samuel Gachuhi Ngugi 2024-10-13 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["introduction.html", "Chapter 2 Introduction 2.1 What is dbt? 2.2 Encounter with dbt 2.3 dbt, from the professionals… 2.4 Why use dbt?", " Chapter 2 Introduction 2.1 What is dbt? dbt, when in full, stands for Data build tool. dbt is a tool that data scientists and analytical engineers use to transform data in their data warehouses. If you are a newbie wanting to, or rather curious about dbt, the preceding statements sure contains a lot. The words analytical engineers, transform data and data warehouses may sound unfamiliar, if not imposing. For now just think of dbt much like a recipe. In a recipe, you have the steps and the instructions to cook your favourite meal, say a roasted chicken. Sure enough, your recipe will contain details on the optimal oven temperature, heating duration and how to set it! dbt works in much the same way. We define how we want to transform or build our data. Once we hit run the magic happens. 2.2 Encounter with dbt How did I come across dbt? Being from a totally different background, the geographical sciences, my first experience with dbt, contrary to what veteran users may say, wasn’t so good. Either because a lot was on my desk back then, but I was having trouble piecing together all the different components that make dbt work, or dbt uses to work. Whichever is the case. It is only after some time, and several hard knocks in between, that I was able to get a semblance of what it does. At least I got a few things. dbt could be used to create views of your tables in the data warehouse, it could perform tests and lastly, (the one I liked the most) it could be used to render a website of your documentation! 2.3 dbt, from the professionals… dbt, from the words of developers, is an open-source tool that analysts and data engineers use to transform data in their data warehouses. Did someone mention transform data somewhere? Data transformation is the process of converting data from its source format to the format required for analysis. The data transformation process is part of a three stage process known as Extract Load and Transform (ELT). Before ELT, Extract Transform Load was the king. The former involves transfering data from the source, to the destination, such as a data warehouse or data lake and performing the transformation in the destination. The latter, though a traditional approach, involves first identifying the data, transforming it prior to landing it to the destination, in this case data warehouse and letting it rest there where downstream users can get hold of it. Here is a better description of the Extract, Load and Transform keywords. Extract - this is the identification and reading of data from one or more sources, such as databases, internet, comma separated value (csv) files and the like. Load - just like you would pull up a weight into a lorry, this is the process of transferring data from the source to your data warehouse. Transform - this is the conversion of data from its state to a format that can be used by downstream users. You may have seen the term data warehouse coming up quite a number of times. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. Examples of data warehouses are Google Big Query, Snowflake, Amazon Redshift, Azure Synapse Analytics, IBM Db2 Warehouse and Firebolt. 2.4 Why use dbt? If you work with data that needs to be version controlled, that is, it can be rolled back to a previous time, you need to work in dbt. If you want to standardize the data models created across teams, dbt is the tool of choice. If you also want a central place where your data work is documented, dbt handles this quite well. In other words, dbt should be the swiss knife when working with large datasets and you want to maintain modularity, order and documentation of your work. The below image summarises the role of dbt in your data processing work. The role of dbt Source: Reference "],["the-dbt-architecture.html", "Chapter 3 The dbt architecture", " Chapter 3 The dbt architecture In my first time working with dbt, I was overwhelmed with its architecture. Similarly to Django, if one didn’t understand a particular component, it would be enough to break your app. Same case with dbt. Sometimes it feels like that individual who is sitting before that large screen in a nuclear power plant and in charge of all the controls. Sometimes it feels like that, like you have to be on top of everything in dbt, like the chap in that nuclear power plant who has to be on top of all the dials and valves. The main components that make up dbt, and which are used in most cases are: models tests documentation sources Let’s go through each one. 3.0.1 Models This is the component of dbt that you will most likely work with. In dbt, a model is simply a SQL statement. As simple as that. dbt will use the SQL statements to perform the transformations in your data warehouse that have been defined in your SQL statement. For example, say I want to create a new column of the table in my Google BigQuery. I will create a SQL statement that does just that. That SQL statement is what is referred to as a model in dbt. Below is an example of a model that creates a table called customers. The model is saved as customers.sql. with customer_orders as ( select customer_id, min(order_date) as first_order_date, max(order_date) as most_recent_order_date, count(order_id) as number_of_orders from jaffle_shop.orders group by 1 ) select customers.customer_id, customers.first_name, customers.last_name, customer_orders.first_order_date, customer_orders.most_recent_order_date, coalesce(customer_orders.number_of_orders, 0) as number_of_orders from jaffle_shop.customers left join customer_orders using (customer_id) 3.0.2 Tests “Do not put me to test”, is a familiar statement we have heard from an already impatient person. However, dbt allows us to test our data and see if it meets certain assertions. In other words, does our data meet the requirements that have been set for it? dbt offers two ways to perform your tests: 1) generic and, 2) custom tests. Generic tests involve just using a pre-defined test that comes packaged in dbt. For example, for every field key you place in a yml file in dbt, you can specify which kind of test to perform on that particular field from the following options: unique, not_null, accepted_values and relationships. unique - the values should be radically distinctive all through not_null - there shouldn’t be a missing value in the particular column name in the table accepted_values - only the values contained in the accepted values key will be considered valid. Anything outside of this will result in an error relationships - the values in this field can be referenced in a different column elsewhere in the table or on a different table altogether. An example of a generic test is below: version: 2 models: - name: orders columns: - name: order_id tests: - unique - not_null - name: status tests: - accepted_values: values: [&#39;placed&#39;, &#39;shipped&#39;, &#39;completed&#39;, &#39;returned&#39;] - name: customer_id tests: - relationships: to: ref(&#39;customers&#39;) field: id For custom tests, these involve one creating a SQL model and referencing it in a yml file using Jinja template language. For example, here is a custom test written a SQL file called transaction_limit_test.sql. -- tests/transaction_limit_test.sql select user_id, sum(transaction_amount) as total_spent from {{ ref(&#39;transactions&#39;) }} group by user_id having total_spent &gt; 10000 -- Assuming the limit is 10,000 The test is referenced in a yml file and called over a column called transactions. models: - name: transactions tests: - transaction_limit_test 3.0.3 Documentation Now the favourite part of dbt, and possibly the easiest is documentation. Documentation is the description of various components of your data. To write a description of any piece of your data, the description key is used. For example here is a description of a field called event_id inside a yml file. version: 2 models: - name: events description: This table contains clickstream events from the marketing website columns: - name: event_id description: The D-day is the Deed day tests: - unique - not_null Documentation will be performed where you have placed your tests. There is also a more complex, but scalable manner of writing descriptions. It uses jinja template tags. It works well for large data where the descriptions are many or the descriptions are shared across several tables. A short example of the jinja templates’ documentation is this. I will write the description in a different file, a markdown file (.md) for the matter, other than the one containing my field names. The descriptions will be like so: {% docs table_events %} I am not so very robust, but I’ll do the best I can. Some text here 1) and here 2) and here 3) and also here {% enddocs %} So when one returns to their yml file, they will reference the particular field of interest with the above description like so: version: 2 models: - name: events description: &#39;{{ doc(&quot;table_events&quot;) }}&#39; columns: - name: event_id description: The D-day is the Deed day tests: - unique - not_null 3.0.4 Sources sources enable one query the data in your datawarehouse. Once you specify the existing table in your datawarehouse under the sources key, you can access every data from within this table using SQL. To work with a source table, you first have to wrap it inside a {{ source(table-name) }} jinja template. Below is an example of declaring a source. version: 2 sources: - name: jaffle_shop database: raw schema: jaffle_shop tables: - name: orders - name: customers - name: stripe tables: - name: payments You can reference the above source inside a SQL model like so: select ... from {{ source(&#39;jaffle_shop&#39;, &#39;orders&#39;) }} left join {{ source(&#39;jaffle_shop&#39;, &#39;customers&#39;) }} using (customer_id) dbt will thereafter know that it will perform some operations using data from the orders and customers data from the jaffle_shop –the origin of all our data in this example. "],["data-storage.html", "Chapter 4 Data storage 4.1 Data warehouse 4.2 Data lake 4.3 Data lakehouse 4.4 A brief history", " Chapter 4 Data storage As as been repeatedly mentioned, to the point of boredom, dbt transforms the data in your data warehouse. Now, before expanding the concept of a data warehouse, the following two are also terms you will here mentioned quite often in the field of analytical engineering. They are data lake and data lakehouse. 4.1 Data warehouse At the very beginning, when introduced to data engineering concepts with a test paper to boot in four weeks time, I thought that a data warehouse was some storage system akin to that found in Google Drive. I could have been partly right, but I was still far off the mark. A data warehouse is more than just a storage system. It is where data is not only stored but also queried, by means of SQL. It allows data from multiple sources such as internet of things, apps, from emails to social media and keeps a historical record of any changes. Examples of data warehouses are Snowflake, Google Big Query, Amazon Redshift and Azure Synapse Analytics. The following are the components of a data warehouse. Data sources - this refers to the origins of the data that lands in your data warehouse. Extract, Transform and Load (ETL) Processes - these are the processes involved in extracting, transforming and loading the data into your data warehouse. Data warehouse database - this is the central repository where the cleansed, integrate and historical data is stored. Metadata repository - metadata is essentially data about data. Metadata will typically contain the source, usage, values and other features that comprise your data. Access tools - imagine having to figure a way how to write a document in your computer without Microsoft Word. How hard would that be? Access tools are similar to Microsoft Word in the aformentioned allegory. These are the tools that enable a user to interact with the data. They include querying, reporting and visualization tools. As you can see from above, a data warehouse is more than just a storage area for your data. It is like a whole community that will provide the services that you desire, so long as they are present in the data warehouse. Source: Reference 4.2 Data lake A data lake is a centralized repository that ingests and stores large volumes of data in its original form. Due to its open, scalable architecture, a data lake can store structured (database tables, excel sheets), semi-structured (xml, json and web pages) and unstructured data (images, audio, tweets) all in one place. Data in the data lake is stored in its original format. So if data lakes and data warehouses store data, then what is the difference? For one, a data lake can store data of any type, so long as it falls within the three classes of structured, semi-structured and unstructured. On the other hand, data warehouses deal with more standardized data. That is, data in a data warehouse has undergone some refinement of some kind to be in a structure that fits the organizations’s goals. Source: Reference 4.3 Data lakehouse A data lakehouse is simply a hybrid of both a data warehouse and data lake. It is like a product that combines the best from both worlds. This is what a data lakehouse provides the following characteristics: scalability of large sums of data from the data lake and the application of a structural schema to data as seen in data warehouses. Even with the above definition, it is still hard to decipher the advantage that a data lakehouse offers above that of a data warehouse. Apart from allowing the querying of unstructured data, storage costs are lower in a data lakehouse compared to a data warehouse. Data lakehouse Source: Reference 4.4 A brief history At the very beginning, companies used to rely on relational databases and these were sufficient. However, they became too numerous as the needs and services of companies grew. Therefore, experts decided to look for a way of how they could merge all these single databases into one repository which would hold everything while allowing for permission controls to who gets access to what. Believe it or not, the concept of the data warehouse began in the 1960s but in the 1980s and 1990s is when it was hot. That’s until the need for storing unstructured data from emails, images and audio began to grow and data warehouses were not so efficient in storing this thanks to… their strict schema enforcement (read, they store data in a structured format). The beginning of 2000s saw the rise in the need to properly manage unstructured data with the growth of online platforms such as Google and Yahoo. Companies needed a way to store and retrieve unstructured data quickly and efficiently, which wasn’t possible with data warehouses. Data lakes excelled in storing all sorts of data, from structured to unstructured and everything in between in their raw format. If you read on the history of data lakes, you will come across the word ‘Hadoop’ quite often. This was the pioneer of the data lakes we use today. However, despite being a good storage for any sort of data, the pesky question of maintaining some quality and order resurfaced again! How could we maintain some structure while allowing the data to be in any structure?! From 2010s and onwards, after a decade of success with data lakes, companies wanted a better storage system from which to run their machine learning models but had the best capabilities of both a data warehouse and a data lake. Before lakehouses, companies would first ingest data into a data lake, then load into a data warehouse from where analytics would be done. But how could we just merge it into one place where storage and analytics could happen? This is how the data lakehouse concept came to be. Data lakehouses provided the following benefits: ACID (Atomicity, Consistency, Isolation and Durability) transactions. ACID transactions promote integrity during data transfer. Delta lake - initially developed by the Databricks team, this is a layer on top of your data in the data lake that provides a schema, keeps a record of changes in your data (versioning) and stores metadata. Machine learning support - because a data lakehouse can store more data types than the data warehouse, it is a better place to perform machine learning modeling. For more information on the evolution of data storage systems, this is a definitive guide. Source: Reference "],["our-data-in-bigquery.html", "Chapter 5 Our data in BigQuery 5.1 Accessing Big Query 5.2 Copying the New York City Bikes data", " Chapter 5 Our data in BigQuery In an earlier chapter, we saw that in data engineering data mainly goes through three processes: extract, load and transform (ELT). The Extract, Transform, Load (ELT) is more of a traditional approach and we will not use it in this case. We will be using Google Bigquery as our data warehouse when working with dbt. As a reminder, let’s go through the definitions of ELT. Extract - the process of identifying and reading data from one or more source systems. We won’t have to do this since the New York City (NYC) bikes data that will be using has already been extracted from whichever the source is by the trusty workers of Google. Load - the process of adding the extracted data to the data warehouse, –in this case Google BigQuery. Again, Google has done this for us. Therefore we won’t have to do it. Transform - the process of converting data from its raw format to the format that it will be use for analysis. This falls definitely within our forte. And we shall use dbt for this. Examples of data transformations that can be done with SQL modeling in dbt are: Replacing codes with values Aggregating numerical sums Applying mathematical functions (SQL can do some maths too, but can be very verbose here) Converting data types Modifying text strings Combining data from different tables and databases. 5.1 Accessing Big Query BigQuery is a data warehouse provided by Google. To access it, open an incognito window and go to this link. Sign in using your gmail account. Click on Console button at the top right. That step of bravery will take you to an interface that looks like this: GCP Interface Click on the dropdown at the top. Select NEW PROJECT. We want to create a new project that will contain some tables that we will work with in dbt. Name your project as dbt-project1 or any other name you prefer. Then select CREATE. Once the project has been created, you will be returned to the original page as first. However, when you select the project dropdown again, you should see your newly created project as one of the options. GCP Project Click on your project, the interface will refresh and the dropdown should now reflect dbt-project1. Click on the Dashboard link on the homepage. The below interface should appear. It can seem overwhelming at first. Dashboard In one of the “boxes” within the Dashboard tab, you will find one called Resources with the BigQuery button underneath. Click on this button. It will take you to a page asking you to Enable the BigQuery Application Programming Interface (API). Kindly comply! Behold, below is the BigQuery interface. BigQuery interface You will see one of the resources as dbt_project1&lt;some-random-number&gt; in case you had other resources. Star this project for quick access in future. 5.2 Copying the New York City Bikes data One of the datasets we will be working with is the “New York City Bikes dataset”. To access it, click on the ADD button. A sidebar will open up. Go to Public Datasets. In the Search Marketplace searchbar, type ‘bikes’. Marketplace Click on the NYC Citi Bike Trips tab. A new sidebar will popup with a button of View Dataset. Click this button and the Google Cloud Platform (GCP) Dashboard will reappear but this time round the bigquery-public-data resource will appear. Click on this particular resource’s dropdown on the left and scroll down to the new_york_citibike dataset. We want to copy this dataset from that of bigquery-public-data to that of dbt_project1-437718. The random numbers will be different in your case. Scroll up again to your dbt_project1 resource. On the kebab menu on the right of this resource, select Create Dataset. Create dataset A new sidebar will open. Insert the following for each parameter: Dataset ID - nyc_bikes Location type - Region Region - africa-south1 (Johannesburg) or your preferred region Thereafter, click on CREATE DATASET. The nyc_bikes dataset should now appear under the dbt-projec1 resource. We want to copy the contents of the new_york_citibike dataset into our nyc_bikes dataset. So how do we proceed? Scroll down to the new_york_citibike dataset under the bigquery-public-data resource and click on it. On the menu for this dataset, you will see the Copy button. Click on this button. Copy dataset Copy sidebar In the Destination searchbar, type nyc_bikes in reference to where we want to copy the contents into. You may need to enable the data transfer API to perform the copy operation. Do so if BigQuery necessitates you that it must be enabled. Once you copy the dataset, a small bar will appear on the screen saying View Details. Click on it to stop the run operation since BigQuery will be rerunning the copy operation after every 24 hours. Disable the transfer process and delete it. Going back to your dbt_project1 resource, your nyc_bikes dataset should now be having two tables under it. That is: citibike_stations citibike_trips Click on any of the tables and preview the data therein using the PREVIEW button of each tables interface. Table Congratulations on loading your first table in BigQuery! "],["installing-dbt.html", "Chapter 6 Installing dbt 6.1 Setting the environment 6.2 Connecting to your BigQuery data warehouse 6.3 Initializing a dbt project", " Chapter 6 Installing dbt Now that we’ve seen how to access a dataset inside our data warehouse, now let’s proceed to installing dbt. This section assumes you have Visual Studio (VS) Code already installed. All code in this book has been done inside a Linux environment but on a Windows computer thanks to the Window Subsystem for Linux (WSL2) virtualization platform. 6.1 Setting the environment Open your VS Code. Create a new folder called dbt_book. Move into this directory in your VS Code by typing cd dbt_book/ in your terminal. The first thing we shall do is create a virtual environment from which we shall conduct all our dbt operations. A virtual environment is useful in preventing conflicts between packages across your various programming projects. python3 -m venv venv The first venv tells python that you’re creating a virtual environment while the second refers to the name of the virtual environment. In this case, our virtual environment shall still share the name venv. Now let’s activate our virtual environment. source venv/bin/activate You will see your namespace appended with venv which means that your virtual environment is now active. For example: (venv) sammigachuhi@Gachuhi:~/dbt_book$ Now here comes the big part: installing dbt for Big Query. We just don’t want to install dbt-core, the dbt packages that we’ll be using, but we also want to install the necessary dependencies that will connect it to BigQuery, where our data is stored. The following code will install everything we need; both dbt and the dependencies needed to connect it to BigQuery. python3 -m pip install dbt-core dbt-bigquery 6.2 Connecting to your BigQuery data warehouse We wish connecting to a data warehouse for dbt were as easy as providing a username and password. However, it is not so. But it is definitely possible. To connect dbt to a data warehouse, we use a keyfile. A keyfile is a file that contains encryption keys or licenses for a particular task. The keyfile we shall use shall be the doorway to our data warehouse. First step, go to your GCP Credentials Wizard page. Ensure that your project is set to the dbt project you created in the previous chapter. For my case, I reverted to an earlier created project called dbt_project since my other project dbt_project1 started incuring costs. For Credential Type: From the Select an API dropdown, choose BigQuery API Select Application data for the type of data you will be accessing Click Next to create a new service account. In the service account page: Type dbt-book as the Service account name or any other name you prefer. From the Select a role dropdown, choose BigQuery Job User and BigQuery Data Editor roles and click Continue Leave the Grant users access to this service account fields blank Once everything is fine it is as good as clicking Done! Your credentials interface will look like below. Service account Click on your service account name. Click on the KEYS tab. We want to create a key that dbt will use to connect to our data warehouse. Click on ADD KEY&gt;Create new key. Add key Select JSON on the interface that appears and click CREATE. This will download a json file containing the encryption keys that dbt will use to connect to your data warehouse. Store this json file in a safe place. 6.3 Initializing a dbt project To create a dbt project, we run the open sesame key: dbt init. It will create a string of outputs. It’s important to key in the right details if you want to create a dbt project. The first output will ask for the name of your dbt project. Insert dbt_book or any other name you prefer. 19:07:31 Running with dbt=1.8.7 Enter a name for your project (letters, digits, underscore): dbt_book If you had an already pre-existing dbt project with the same name, dbt will ask if it can overwrite that project. Type y if you wish to do so. dbt will thereafter ask you which database you would like to use. Since we had installed dbt with the package dependancies for BigQuery, you will see the sole option for BigQuery. Type 1 to select BigQuery. Which database would you like to use? [1] bigquery (Don&#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) Enter a number: 1 You will thereafter be asked the authentication method you would like to use. Since we had already created a service account and downloaded the JSON file containing the encryption keys, we shall select option 2. Enter a number: 1 [1] oauth [2] service_account Desired authentication method option (enter a number): 2 For the keyfile, provide the path to where you had saved the json file. As a note, this path should be somewhere different than where your dbt project is located. This is because saving the project into Github with your json keys as part of its files will cause Github to raise an alarm and send consistent emails. This is because the json keys are never meant to be shared or stored somewhere accessible. They are considered sensitive information keyfile (/path/to/bigquery/keyfile.json): /home/sammigachuhi/dbt_credentials/dbt_book.json You will also be asked to provide your project ID. This is available under your dbt project’s dashboard under the Project ID heading. project (GCP project id): dbt-project-437116 For the dataset name, we will use nyc_bikes which is the dataset we want to conduct our dbt operations on. dataset (the name of your dbt dataset): nyc_bikes For the rest of the options, you can fill them as below: threads (1 or more): 1 job_execution_timeout_seconds [300]: [1] US [2] EU Desired location option (enter a number): 1 19:09:24 Profile dbt_book written to /home/sammigachuhi/.dbt/profiles.yml using target&#39;s profile_template.yml and your supplied values. Run &#39;dbt debug&#39; to validate the connection. Now, in order to test whether your dbt installation is correct, you will have to change directory into your dbt_book subfolder we created as part of the dbt init prompts. At first, we had created a directory called dbt_book in which we also activated the virtual environment. When we ran dbt init from this directory, we specified our project name to be dbt_book as well. It is from here we want to check if our dbt initialization and access to BigQuery was successful. So move into this subfolder via cd dbt_book/. (venv) sammigachuhi@Gachuhi:~/dbt_book$ cd dbt_book/ (venv) sammigachuhi@Gachuhi:~/dbt_book/dbt_book$ dbt debug Inside the dbt_book subfolder we created as part of the dbt initialization prompts, run dbt debug. If the final output of the run is All checks passed!, you are good to go! 19:10:20 Running with dbt=1.8.7 19:10:20 dbt version: 1.8.7 19:10:20 python version: 3.10.12 19:10:20 python path: /home/sammigachuhi/dbt_book/venv/bin/python3 19:10:20 os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 19:10:21 Using profiles dir at /home/sammigachuhi/.dbt 19:10:21 Using profiles.yml file at /home/sammigachuhi/.dbt/profiles.yml 19:10:21 Using dbt_project.yml file at /home/sammigachuhi/dbt_book/dbt_book/dbt_project.yml 19:10:21 adapter type: bigquery 19:10:21 adapter version: 1.8.2 19:10:22 Configuration: 19:10:22 profiles.yml file [OK found and valid] 19:10:22 dbt_project.yml file [OK found and valid] 19:10:22 Required dependencies: 19:10:22 - git [OK found] 19:10:22 Connection: 19:10:22 method: service-account 19:10:22 database: dbt-project-437116 19:10:22 execution_project: dbt-project-437116 19:10:22 schema: nyc_bikes 19:10:22 location: US 19:10:22 priority: interactive 19:10:22 maximum_bytes_billed: None 19:10:22 impersonate_service_account: None 19:10:22 job_retry_deadline_seconds: None 19:10:22 job_retries: 1 19:10:22 job_creation_timeout_seconds: None 19:10:22 job_execution_timeout_seconds: 300 19:10:22 timeout_seconds: 300 19:10:22 client_id: None 19:10:22 token_uri: None 19:10:22 dataproc_region: None 19:10:22 dataproc_cluster_name: None 19:10:22 gcs_bucket: None 19:10:22 dataproc_batch: None 19:10:22 Registered adapter: bigquery=1.8.2 19:10:26 Connection test: [OK connection ok] 19:10:26 All checks passed! "],["models-1.html", "Chapter 7 Models 7.1 Running a model 7.2 Model structure 7.3 A custom model", " Chapter 7 Models If you have gone through previous chapters, you will by now know that a model in dbt is any SQL file. It is what dbt will use to build tables, views and any other transformations in your data warehouse (read BigQuery). In dbt, models are executed with the hit and run command: dbt run. 7.1 Running a model dbt did us a very big favour during installation, it came with two models already created for us. These are namely the my_first_dbt_model.sql and my_second_dbt_model.sql within the models/example directory. It also provided a schema.yml file within the same directory which provides definitions for the model’s schema. Alright. Assuming that you are within the dbt_book subdirectory and your virtual environment (venv) already activated, type the following in your terminal dbt run. (venv) sammigachuhi@Gachuhi:~/dbt_book/dbt_book$ dbt run This will initiate a series of printouts. However, before we go to the expected output, you may run into an error related to the location not being found. 404 Not found: Dataset dbt-project-437116:nyc_bikes was not found in location US; reason: notFound, message: Not found: Dataset dbt-project-437116:nyc_bikes was not found in location US When we were initializing our project using dbt init we selected option 1 for US since there was no other option apart from EU. Luckily, there is a work around to this. It involves editing the projects.yml file. If you run dbt debug it will also show the path of your projects.yml alongside other configuration information. 18:19:56 Running with dbt=1.8.7 18:19:56 dbt version: 1.8.7 18:19:56 python version: 3.10.12 18:19:56 python path: /home/sammigachuhi/dbt_book/venv/bin/python3 18:19:56 os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 18:19:57 Using profiles dir at /home/sammigachuhi/.dbt 18:19:57 Using profiles.yml file at /home/sammigachuhi/.dbt/profiles.yml --snip-- Go to the provided path for profiles.yml which is found at /home/sammigachuhi/.dbt in my case. Open it and change the line with location to read from US to africa-south1. dbt_book: outputs: dev: dataset: nyc_bikes job_execution_timeout_seconds: 300 job_retries: 1 keyfile: /home/sammigachuhi/dbt_credentials/dbt_book.json location: africa-south1 --snip--- Now come back, and rerun dbt run again. dbt should now be able to run against your warehouse and create a table called my_first_dbt_model and a view my_second_dbt_model in BigQuery. “How do I know that my queries ran successfully?”, you may ask. One, the output of a successful dbt run is Completed successfully. Beneath this, message, will be a log of the number of models ran and if there have been any errors. We had two models, so we expect to see this reflect in the log. And it did. 18:21:16 Running with dbt=1.8.7 18:21:17 Registered adapter: bigquery=1.8.2 18:21:17 Unable to do partial parsing because saved manifest not found. Starting full parse. 18:21:19 Found 2 models, 4 data tests, 479 macros 18:21:19 18:21:22 Concurrency: 1 threads (target=&#39;dev&#39;) 18:21:22 18:21:22 1 of 2 START sql table model nyc_bikes.my_first_dbt_model ...................... [RUN] 18:21:30 1 of 2 OK created sql table model nyc_bikes.my_first_dbt_model ................. [CREATE TABLE (2.0 rows, 0 processed) in 7.83s] 18:21:30 2 of 2 START sql view model nyc_bikes.my_second_dbt_model ...................... [RUN] 18:21:34 2 of 2 OK created sql view model nyc_bikes.my_second_dbt_model ................. [CREATE VIEW (0 processed) in 4.01s] 18:21:34 18:21:34 Finished running 1 table model, 1 view model in 0 hours 0 minutes and 14.88 seconds (14.88s). 18:21:34 18:21:34 Completed successfully 18:21:34 18:21:34 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 The second way, and the most obvious, is checking the results in your BigQuery data warehouse. If you see the table and view my_first_dbt_model and my_second_dbt_model respectively, the query ran successfully. Models It was that simple, isn’t it? 7.2 Model structure Let’s take a look at the model my_first_dbt_model.sql. {{ config(materialized=&#39;table&#39;) }} with source_data as ( select 1 as id union all select null as id ) select * from source_data /* Uncomment the line below to remove records with null `id` values */ -- where id is not null The line {{ config(materialized='table') }} tells dbt to make the output my_first_dbt_model as a table. Any configurations set at the model level will overide the overarching ones set at dbt_project.yml file. If you quickly take a sneek peek at the dbt_project.yml file and scroll to the bottom, you will see this configuration: models: dbt_book: # Config indicated by + and applies to all files under models/example/ example: +materialized: view This configuration simply says that for every model inside the dbt_book/example directory, as displayed by the new line and identations, materialize the result as a view1. However, inside our my_first_dbt_model.sql file, we set the materialization as table. What is in the sql model will override what is in the dbt_project.yml. However, for my_second_dbt_model.sql, we didn’t specify the materialization. Like the case of the rat reigning in the cat’s absence, the materialization specified in the dbt_project.yml will call the shots for the second model. That’s why for my_second_dbt_model the materialization is a view, and not a table as its counterpart. Materialization is the persistence of a model in the data warehouse. The second part of our first model is the actual SQL statement. with source_data as ( select 1 as id union all select null as id ) select * from source_data This is a WITH SQL statement. In very simple terms, the SQL statement in parentheses () is what is referenced as source_data. Once we define our SQL statement and close it with parentheses, we can now select the data referenced by source_data. The SQL statements in parentheses is what is referred to as Common Table Expression (CTE). They were designed to simplify complex queries and be used in the context of a larger query. The second model doesn’t have much to provide but it introduces a new trick: the ref function. select * from {{ ref(&#39;my_first_dbt_model&#39;) }} where id = 1 The ref function is part of the jinja templating language. It is used to reference a model that has been provided within the parentheses enclosed with quotes (''). Therefore, in essence, our model is simply returning the row(s) from my_first_dbt_model that contain the value 1 in the id column. Here is the result of my_second_dbt_model view when I query BigQuery to show its contents. Second model 7.3 A custom model Now that we have seen how to run our models, and we have hit the ground running with some of the models preshipped with dbt, it’s now time to fold our shirts and run our own custom model. Let’s start easy. We will be no means delve into a complex model since that is only limited by your imagination. If you know SQL, there is no limit to the models, both in number and complexity, that you can do in dbt. Our first model will endeavour to perform a change on the citi_trips table within our nyc_bikes dataset. There is a very ripe column called tripduration which stands for the trip in second every cab ride took. Humans prefer to work in minutes so a trivial dbt job would involve creating a column that shows the tripduration in minutes. dbt would then be doing the Transform part in the ELT. In the citi_trips_minutes.sql model, we have written a code that creates a view of this result. Remember, views are just virtual tables but they do save on storage! {{ config(materialized=&#39;view&#39;) }} WITH citi_trips AS ( SELECT *, tripduration / 60 AS trip_duration_min FROM `dbt-project-437116.nyc_bikes.citibike_trips` ) SELECT * FROM citi_trips The above SQL statement is also a form of Common Table Expression (CTE). It’s not that hard. The variable citi_trips just references the SQL statement in parentheses. There is another small trick of the trade. What if this very simple dbt model was just part of thousands, and longer running SQL models. If we wanted to just run this small, tiny winy model, would it have to be part of the entourage of the other model runs. Such would be a waste of time and resources, assuming some of them are heavy models. No. We can use the --select keyword to only select the model we want to run. Below we run the citi_trips_minutes model. dbt run --select models/example/citi_trips_minutes.sql Below is the output. Concurrency: 1 threads (target=&#39;dev&#39;) 18:50:33 18:50:33 1 of 1 START sql view model nyc_bikes.citi_trips_minutes ....................... [RUN] 18:50:37 1 of 1 OK created sql view model nyc_bikes.citi_trips_minutes .................. [CREATE VIEW (0 processed) in 4.53s] 18:50:37 18:50:37 Finished running 1 view model in 0 hours 0 minutes and 8.17 seconds (8.17s). 18:50:37 18:50:37 Completed successfully 18:50:37 18:50:37 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 We are always glad when we see the soothing words “Completed successfully”. If you go to BigQuery, our data warehouse, you will see a new view called citi_trips_minutes already created. By default, the name of the newly formed view or table in the data warehouse will be that of the model used to create it. Similar to the above, the below run command will also work. dbt run --select citi_trips_minutes But how do we view this newly created result. Unlike a table, BigQuery does not offer the PREVIEW button. However, there is a way… Query Click on the Query button, select In new tab and a new tab will form. Copy this SQL query onto the tab and run it to display the result of our citi_trips_minutes view. SELECT * FROM `dbt-project-437116.nyc_bikes.citi_trips_minutes` Our trip_duration_min column of interest is onto the far right of our view. We had hinted earlier that there is no limit to the complexity or number of models you can create in dbt. Below is a sort of more complex model (but not much) compared to the earlier one. This model rounds of the minutes to one decimal place and also removes the null columns, thus reducing number of rows from 58, 937, 715 to 53, 108, 721. The model is saved as citi_trips_round.sql. {{ config(materialized=&#39;view&#39;) }} WITH citi_trips_round AS ( SELECT *, ROUND(trip_duration_min, 1) AS trip_min_round FROM ( SELECT *, tripduration / 60 AS trip_duration_min FROM `dbt-project-437116.nyc_bikes.citibike_trips` ) WHERE tripduration IS NOT NULL ) SELECT * FROM citi_trips_round Our model is saved as the citi_trips_round view in BigQuery. Create a new query tab in BigQuery to view the results. Also use the query tab to count the number of rows and compare that with those from the citibike_trips table. It’s a difference of 5.8 million rows of null value eliminated. SELECT * FROM `dbt-project-437116.nyc_bikes.citi_trips_round`; SELECT COUNT(*) FROM `dbt-project-437116.nyc_bikes.citi_trips_round`; -- This is the count of the original citibike_trips table SELECT COUNT(*) FROM `dbt-project-437116.nyc_bikes.citibike_trips`; You may be wondering what the advantage that dbt provides. After all, can’t one just use the SQL Query tab in BigQuery just to do the transformations and save the table? Fine, that can work. However, dbt offers a form of persistence to your models. You see, an SQL query tab can be erased or modified, but the SQL queries in dbt even if they too can be modified at will they are persisted longer if they are saved in a versioning environment such as Github. One more thing, you don’t have to use the models/example folder. You can choose to rename this one, or save the two models we created in a different folder. They will still run fine. In fact, like a stubbon mathematician who will want to battle-test his concepts though they are as robust as Zeus, let’s do it. Create a new folder within the models directory called my_models. Move the citi_trips_minutes and citi_trips_round models into the my_models directory. Thereafter, run these two models using dbt run --select models/my_models. The should run fine. As an extra bit of information, all successfull compile and run models will appear under the /target directory. A view is a virtual table, similar to the original table, the physical dataset it was created from↩︎ "],["documentation-1.html", "Chapter 8 Documentation 8.1 The yml files 8.2 Definition for our model 8.3 Using the doc function 8.4 Images in dbt documentation 8.5 Generating the document", " Chapter 8 Documentation In the book The voyages and adventures of Captain Hatteras the survivors of a ship whose expedition was to the North Pole relied on the writing of former captains, explorers and sailsmen to not only find the best possible route to the North Pole, but also the hazards and the places where coal was hidden for future explorers like they who were keen on finding the coveted North Pole. So how does this tie to data engineering and dbt? Well, in any digital organization, there is sure to be some turnover. There is sure to be some new chap who would want to wrap their heads around what the organization was doing, and the data was using. Documentation is one way to enable these experts start on a sure footing, but it is rarely the norm. The good thing with dbt is that it provides a way to create documentation at the same place you write code to transform it, not in a separed pdf2 as we all do! 8.1 The yml files In dbt, yml files can do a lot of things. One of the stuff it does is documentation and creation of tests for your data. But first, here are the rules of writing a yml file. Indents should be two spaces List items should be indented Use a new line to separate list items that are dictionaries where appropriate For this case, we shall use the yml files to create documentation for our data. There is hardly any information on a rule-based methodology to create a yml file. However, for documentation and testing purposes, we already have a template to start us off with. This is the schema.yml file inside the models/example directory. version: 2 models: - name: my_first_dbt_model description: &quot;A starter dbt model&quot; columns: - name: id description: &quot;The primary key for this table&quot; data_tests: - unique - not_null - name: my_second_dbt_model description: &quot;A starter dbt model&quot; columns: - name: id description: &quot;The primary key for this table&quot; data_tests: - unique - not_null Let’s go through the above structure briefly. 8.1.1 version: 2 There is a story behind this. It is that at the very beginning of dbt development, the structure was very different and inserting version: 2 enabled developers know which version of dbt they were working. Don’t expect a version: 3 to come any time soon, but this is just a required necessity. 8.1.2 models Remember when we said that a model in dbt is simply a sql file? Well, next to the name key which is under this key you specify the name of your sql file, minus the .sql extension. 8.1.3 description This is where you insert the description of your table. 8.1.4 columns These are the fields contained in your table. You name them here and under each column are three mappings. name - this is the name of the field in your table. In other words, it is the column name. description - this is a short explanation of your column. data_test - the kind of tests that you would like to perform on your field are inserted here. dbt comes with generic tests such as unique, not_null and integer but you can create your own custom tests too. 8.2 Definition for our model Alright, having gone through the template, we can create our own yml file under the my_models directory. Let’s call it my_models.yml. Copy past the yml structure from schema.yml to the my_models.yml and let the first yml structure for citi_trips_minutes.sql look like below. version: 2 models: - name: citi_trips_minutes description: &quot;This is a table with an extra column showing the trip duration in minutes&quot; columns: - name: tripduration description: &quot;Trip Duration (in seconds)&quot; - name: starttime description: &quot;Start Time, in NYC local time.&quot; - name: stoptime description: &quot;Stop Time, in NYC local time.&quot; - name: start_station_id description: &quot;Start Station ID&quot; - name: start_station_name description: &quot;Start Station Name&quot; - name: start_station_latitude description: &quot;Start Station Latitude&quot; - name: start_station_longitude description: &quot;Start Station Longitude&quot; - name: end_station_id description: &quot;End Station ID&quot; - name: end_station_name description: &quot;End Station Name&quot; - name: end_station_latitude description: &quot;End Station Latitude&quot; - name: end_station_longitude description: &quot;End Station Longitude&quot; - name: bike_id description: &quot;Bike ID&quot; - name: usertype description: &quot;User Type (Customer = 24-hour pass or 7-day pass user, Subscriber = Annual Member)&quot; - name: birth_year description: &quot;Year of Birth&quot; - name: gender description: &quot;Gender (unknown, male, female)&quot; - name: customer_plan description: &quot;The name of the plan that determines the rate charged for the trip&quot; - name: trip_duration_min description: &quot;The trip duration in minutes&quot; What we’ve done is quite straightforward. We have simply typed out the descriptions in a verbose manner where they needed to be, next to the description mapping key. However, imagine you were working with hundreds of models which use similar definitions. Would you have the nerve to copy paste every definition to its respective model? I don’t think so. There is a function by the name of the docs() function which can reference to descriptions in a separate markdown file. 8.3 Using the doc function To use the doc() function, we write our definitions in a separate markdown (.md) file and place the descriptions within {% docs &lt;field-name&gt; %} {% enddocs %} tags. For this tutorial, we created three markdown tables. references.md - this contains the descriptions for our column names of interest tables.md - contains the descriptions for our tables of interest overview.md - contains the text that will go to the overview page Here is what our references.md contains. As you can see, we have provided some textual information for some of our column names. We can also add some more style to our descriptions since they are now on a separate markdown. For example we could insert links, make the text italic, and bold if you wish! {% docs tripduration %} Trip Duration (in seconds). Like: - How long did the trip take? - What is the time in seconds? - More info on time, see [here](https://www.poemhunter.com/poem/time-xxi/) *https://www.poemhunter.com/poem/time-xxi/* {% enddocs %} {% docs starttime %} Start Time, in NYC local time. As accurate as could ever be. {% enddocs %} --snip-- The tables.md just contains a description of our citi_trips_round table. {% docs citi_trips_round %} This table contains the trip duration in minutes to one decimal place only. {% enddocs %} Now, in order to enable dbt reference these descriptions from our yml file, we would simply use the doc () function enclosed in single quotes and curly brackets like so: - name: citi_trips_round description: &#39;{{ doc(&quot;citi_trips_round&quot;) }}&#39; columns: - name: tripduration description: &#39;{{ doc(&quot;tripduration&quot;) }}&#39; - name: starttime description: &#39;{{ doc(&quot;starttime&quot;) }}&#39; - name: stoptime description: &#39;{{ doc(&quot;stoptime&quot;) }}&#39; - name: start_station_id description: &quot;Start Station ID&quot; --snip-- The file saved as overview.md in our project will be used to display the home page of our dbt documentation website. The markdowns created so far, including the one for overview, can go with any other name provided the wordings in the docs tags are reference correctly. A rose by any other name is still a rose! Back to the overview page. The overview page is simply the home page, but in dbt we use a slightly different syntax for the overview page, like so: {% docs __overview__ %} Some more text here... {% enddocs %} Therefore, here is some dummy text for our overview page. {% docs __overview__ %} # Learning dbt Learning is not merely the acquisition of knowledge, but the cultivation of the mind. It is through the active engagement of our intellect that we develop the capacity for critical thought, discernment, and wisdom. --snip-- {% enddocs %} 8.4 Images in dbt documentation They say an image is worth a thousand words. Reading plain text can be boring, and saving it efficiently can also be painstaking. In dbt, we store images in a folder called assets. Ideally, one can create any folder in dbt to store images provided you reference it correctly in the documentation. However, it is preferably and strongly advised by the dbt team to store it inside an assets folder for versioning purposes. Furthermore, images in dbt, once run as part of your document generation, will also appear under the targets/ folder just like your SQL models. Therefore, going with the recommended approach, create an assets/ folder under dbt_book. Place you image in there. Go to the dbt_projects.yml file and create a new line with the following code: asset-paths: [&quot;assets&quot;] This path tells dbt to copy any images within assets into the target directory. Any image in a different directory will not get copied into the target directory when documents are generated. Finally, as the missing piece to the puzzle, insert a reference to your image in the overview.md file. ![Example image](assets/image_example.jpg) One can also create custom overviews for the dbt packages they used. However, since we didn’t use them, they weren’t placed here. Here is our complete overview.md file. {% docs __overview__ %} # Learning dbt Learning is not merely the acquisition of knowledge, but the cultivation of the mind. It is through the active engagement of our intellect that we develop the capacity for critical thought, discernment, and wisdom. By examining the world around us with curiosity and rigor, we uncover the underlying principles that govern its workings. This intellectual pursuit not only broadens our understanding but also equips us to navigate life&#39;s challenges with greater clarity and purpose. ![Example image](assets/image_example.jpg) Some more text here... {% enddocs %} 8.5 Generating the document Now is the time where we ignite the rocket engines and shoot off. To generate a dbt documentation, first run dbt docs generate. This command tells dbt to compile the necessary information of your project into the catalog.json and manifest.json files. The ignition key for our documentation generation is dbt docs serve. dbt will generate a list of outputs and create a popup providing the browser link to open up your documentation. You can click on the popup or copy-paste the link. In this case, we used the former. Our documentation is in the host: localhost:8080/. Model page Models page If you go to the targets directory, our image(s) will be there! There is also one more cool functionality of the dbt documentation. On the bottom right, there is a turquoise button for showing the lineage graph for each model. If you click on any model, such as my_second_dbt_model, you will see it shows a dependency on my_first_dbt_model. If you have worked on a model that has several dependencies, or children, the model will most likely be more complex. A good example will be for the citi_trips_long model. Lineage graph It is highly encouraged to play around with the buttons resources, packages, tags, –select and –exclude. For the select button, play around with inserting + both before and after the name of the model. Clue: it has to do with showing or hiding the dependencies and/or children. Below is an example of the citi_trips_long model, which has more than one dependency. A model with more than one dependency Portable Document Format↩︎ "],["tests-1.html", "Chapter 9 Tests 9.1 Types of tests in dbt 9.2 Generic tests in dbt 9.3 Singular tests in dbt 9.4 Creating a generic test 9.5 Configuring custom generic tests 9.6 Storing test failures", " Chapter 9 Tests Probably if you’ve worked on a Windows computer, you must have used Microsoft Defender Full Scan at some point. During or at the end of the scan, some results were displayed. dbt works in almost the same way, only that this time they scan your data. Tests in dbt are basically assertions of your data. That is, they are just some assumptions that you have of your datasets or columns that are correct. Tests enable one to know 1) which assumptions are wrong about our data, and 2) which parts of our data diverge from the expected norm. Tests can sometimes feal like something to bemoan, can fail (as all tests do) but the overarching advantage is that they make us understand more about our data. They can also be lifesavers in that they can pinpoint a serious problem which could be harder to debug if not more injurious to the integrity of your data later on! In a nutshell, dbt tests perform much like a programmatic scan that will only spew out errors of something that is suspicious. 9.1 Types of tests in dbt In dbt, a test can be defined in either of the following two ways: generic test - this is a test written in a SQL model and defined inside a YAML file. The test in the YAML file is defined using jinja Macros. dbt comes with four shipped, all-batteries included tests namely: unique - asserts that the column has no repeating values not_null - asserts that there are no null values accepted_values - checks if the values in your field correspond to those in a defined list relationships - checks if the field has an existing relationship with another field in a different table. singular test - some normally refer to this as a custom test. This is a SQL query which is used to check assertions in your data. The SQL files that make up your test are defined inside the tests directory or the path defined in the test-paths key inside the dbt_project.yml file. Each SQL file will have one test only. Nevertheless, if this test will be used across many fields and files, you can reference it using jinja macros {{ }}. If this is the case, it is no longer singular test but a generic custom test. Let’s start with a dbt out-of-the-box generic test. 9.2 Generic tests in dbt We will start with a very simple test, the not_null test on the start_station_name column of citi_trips_long model. Surely, unless you are teleporting from somewhere, every rented bike must have an origin. - name: citi_trips_long description: &#39;{{ doc(&quot;citi_trips_long&quot;) }}&#39; columns: - name: starttime description: &#39;{{ doc(&quot;starttime&quot;) }}&#39; --snip-- - name: start_station_name description: &quot;Start Station Name&quot; tests: - not_null We use the below code to test only those models under my_models folder. dbt test --select my_models Here is the output. --snip-- 19:15:33 Concurrency: 1 threads (target=&#39;dev&#39;) 19:15:33 19:15:33 1 of 1 START test not_null_citi_trips_long_start_station_name .................. [RUN] 19:15:36 1 of 1 PASS not_null_citi_trips_long_start_station_name ........................ [PASS in 3.20s] 19:15:36 19:15:36 Finished running 1 test in 0 hours 0 minutes and 4.44 seconds (4.44s). 19:15:36 19:15:36 Completed successfully 19:15:36 19:15:36 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 If we had gone with the more blanket code of dbt test, it would have also tested those models under the shipped examples folder where there is already a test designed to fail, purely for educational purposes. Back to our citi_trips_long model, we can see that the test passed successfully. As expected, there are no null values in our start_station_name field. If the opposite happened, we suspect that this could be a case of fraud or some paranormal activities… Now let’s create a test that will fail on purpose. From a large dataset, obviously the station names can’t be unique all through. So we insert the unique value under the tests key as follows. - name: start_station_name description: &quot;Start Station Name&quot; tests: - not_null - unique The output is as below. Note that it results in a failure and also shows the path to the SQL query that was used to carry out the test inside the target/ directory. The resultant SQL path inside the target directory, if pasted in a SQL tab of your data warehouse will return the number of rows that fail the test. 19:23:35 Concurrency: 1 threads (target=&#39;dev&#39;) 19:23:35 19:23:35 1 of 2 START test not_null_citi_trips_long_start_station_name .................. [RUN] 19:23:38 1 of 2 PASS not_null_citi_trips_long_start_station_name ........................ [PASS in 3.01s] 19:23:38 2 of 2 START test unique_citi_trips_long_start_station_name .................... [RUN] 19:23:41 2 of 2 FAIL 910 unique_citi_trips_long_start_station_name ...................... [FAIL 910 in 2.68s] 19:23:41 19:23:41 Finished running 2 data tests in 0 hours 0 minutes and 6.78 seconds (6.78s). 19:23:41 19:23:41 Completed with 1 error and 0 warnings: 19:23:41 19:23:41 Failure in test unique_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:23:41 Got 910 results, configured to fail if != 0 19:23:41 19:23:41 compiled code at target/compiled/dbt_book/models/my_models/my_models.yml/unique_citi_trips_long_start_station_name.sql 19:23:41 19:23:41 Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2 Failed test 9.3 Singular tests in dbt As mentioned earlier, singular tests are SQL models in your tests folder. They differ from the generic tests in that they are bespoke to your data needs. For example, if one of your tests doesn’t conform to the four batteries included tests, you can create one inside the tests folder. Below we create a singular test called unnecessary_trips.sql inside the test folder. The purpose of the test is to raise an error if a trip is less than 1 min, and if this passes without a hitch, we increase the trip to 10 min. The latter must surely generate an error! Notice that one can reference another model within a test using the ref() function. SELECT bikeid, start_station_name, end_station_name, birth_year, gender, tripduration FROM {{ ref(&#39;citi_trips_round&#39;) }} WHERE trip_min_round &lt; 10 So to perform the litmus test, run our usual magic phrase: dbt test --select unnecessary_trips Below will be the output. 19:51:50 Concurrency: 1 threads (target=&#39;dev&#39;) 19:51:50 19:51:50 1 of 1 START test unnecessary_trips ............................................ [RUN] 19:51:53 1 of 1 FAIL 25183763 unnecessary_trips ......................................... [FAIL 25183763 in 3.11s] 19:51:53 19:51:53 Finished running 1 test in 0 hours 0 minutes and 4.70 seconds (4.70s). 19:51:53 19:51:53 Completed with 1 error and 0 warnings: 19:51:53 19:51:53 Failure in test unnecessary_trips (tests/unnecessary_trips.sql) 19:51:53 Got 25183763 results, configured to fail if != 0 19:51:53 19:51:53 compiled code at target/compiled/dbt_book/tests/unnecessary_trips.sql 19:51:53 19:51:53 Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1 As you can see, a total 25183763 rows failed the test. If we had used a condition of less than 1 min (trip_min_round &lt; 1) the test would have passed. A singular test can also be transformed into a generic test when its reused across the fields of your table(s) in the data warehouse. To demonstrate this, let’s create some generic tests. 9.4 Creating a generic test From the dbt documentation website, your custom generic tests are created within a generic folder within the tests directory. Thus, within your tests/generic directory, you will place the SQL models for your tests. Anything returned by your SQL models is in fact your tests failing! If nothing is returned, then your test passed, and your data as clean as a new pin. Create a SQL model called long_characters within the tests/generic directory. {% test long_characters (model, column_name) %} SELECT * FROM {{ model }} WHERE LENGTH({{ column_name }}) &gt; 15 {% endtest %} Let’s go through the above line by line. We begin a generic test by using the {%test &lt;model-name&gt; (model, column_name) %} SQL statement in here {% endtest %} tags. The model and column_name are standard arguments where one or both should be defined. model - the resource on which the test will be operated on. In our case, this is any model in which the test will be run on. column_name - this is a field within which the model will be run against. In other words, the column at which this model will operate on. The SQL statement within the test tags references the model and the columns using the {{ model }} and {{ column_name }} respectively. For example, if the test is placed in the my_models YAML file under the citi_trips_long model name, for the field start_station_name, it is as though the test is running this SELECT statement: SELECT * FROM citi_trips_long WHERE LENGTH(start_station_name) &gt; 15 In very few words, the above SQL tells dbt to shout out an error if any station name is greater than 15 characters. Such must be train stations larger than life. Let’s create another test that will raise alarms if there are special characters within your start_station_name. {% test special_characters (model, column_name) %} SELECT * FROM {{ model }} WHERE {{ column_name }} LIKE &#39;%^[a-zA-Z0-9+-]%&#39; {% endtest %} Okay, it’s not time for our litmus test. Going back to the start_station_name mapping that we have performed some litmus tests, lets also add our two custom generic tests of long_character and special_characters. - name: citi_trips_long description: &#39;{{ doc(&quot;citi_trips_long&quot;) }}&#39; columns: --snip-- - name: start_station_name description: &quot;Start Station Name&quot; tests: - not_null - unique - long_characters - special_characters If we go for the big bull and run all our tests with trusty dbt test keyword, we can see the output of the nine tests we have so far. 19:08:15 Concurrency: 1 threads (target=&#39;dev&#39;) 19:08:15 19:08:15 1 of 9 START test long_characters_citi_trips_long_start_station_name ........... [RUN] 19:08:29 1 of 9 FAIL 11463868 long_characters_citi_trips_long_start_station_name ........ [FAIL 11463868 in 14.47s] --snip-- 19:08:55 5 of 9 START test special_characters_citi_trips_long_start_station_name ........ [RUN] 19:08:59 5 of 9 PASS special_characters_citi_trips_long_start_station_name .............. [PASS in 4.48s] --snip-- 19:09:07 9 of 9 START test unnecessary_trips ............................................ [RUN] 19:09:10 9 of 9 FAIL 25183763 unnecessary_trips ......................................... [FAIL 25183763 in 2.81s] From the above output, we can see that there were some station names with quite some long names and (thankfully) no station names with special characters. There is also other output below the above which shows how many records failed, depending on your test. 19:09:10 Completed with 4 errors and 0 warnings: 19:09:10 19:09:10 Failure in test long_characters_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:09:10 Got 11463868 results, configured to fail if != 0 --snip-- 19:09:10 Failure in test unique_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:09:10 Got 910 results, configured to fail if != 0 9.5 Configuring custom generic tests What if you feel that the FAIL alert like in the above tests is shouting too much! That you would prefer them to be WARNINGS because they are non-critical? You can do so by setting the configuration to diplay as a warning rather than an error. {% test long_characters (model, column_name) %} {{ config(severity = &#39;warn&#39;) }} SELECT * FROM {{ model }} WHERE LENGTH({{ column_name }}) &gt; 15 {% endtest %} Here is the output as a warning. 19:16:34 1 of 9 START test long_characters_citi_trips_long_start_station_name ........... [RUN] 19:16:36 1 of 9 WARN 11463868 long_characters_citi_trips_long_start_station_name ........ [WARN 11463868 in 2.64s] 19:16:36 2 of 9 START test not_null_citi_trips_long_start_station_name .................. [RUN] 19:16:39 2 of 9 PASS not_null_citi_trips_long_start_station_name ........................ [PASS in 2.67s] -- snip -- 19:16:56 Warning in test long_characters_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:16:56 Got 11463868 results, configured to warn if != 0 -- snip -- However, in case you immediately change your mind that the failing tests of long_characters returned should be an error rather than a warning, you can simply overide your custom generic SQL models by specifying the severity within the YAML definition files. - name: start_station_name description: &quot;Start Station Name&quot; tests: - not_null - unique - long_characters: severity: &#39;error&#39; - special_characters In the generated output, it will be back to business as usual with the ‘FAIL’ keyword. -- snip -- 19:21:25 1 of 9 START test long_characters_citi_trips_long_start_station_name ........... [RUN] 19:21:28 1 of 9 FAIL 11463868 long_characters_citi_trips_long_start_station_name ........ [FAIL 11463868 in 3.45s] -- snip -- 19:21:50 Failure in test long_characters_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:21:50 Got 11463868 results, configured to fail if != 0 -- snip -- 9.6 Storing test failures If you thought that dbt tests are a cool feature, then there is one more trick in the bag if you want a neat one-liner to view a dataset of the failing records. The open sesame key word is --store-failures. dbt will store the failing records as a table in the database. Let’s try it out. This is only what is needed to be run. dbt test --store-failures Of course our test will generate failed records, but we’ve already seen them. Here is part of the output for the test of long characters above the 15 character threshold. 19:26:40 Failure in test long_characters_citi_trips_long_start_station_name (models/my_models/my_models.yml) 19:26:40 Got 11463868 results, configured to fail if != 0 19:26:40 19:26:40 compiled code at target/compiled/dbt_book/models/my_models/my_models.yml/long_characters_citi_trips_long_start_station_name.sql 19:26:40 19:26:40 See test failures: ------------------------------------------------------------------------------------------------------------------- select * from `dbt-project-437116`.`nyc_bikes_dbt_test__audit`.`long_characters_citi_trips_long_start_station_name` ------------------------------------------------------------------------------------------------------------------- If you paste the SELECT statement in one of the SQL tabs for BigQuery, it will not only return the number of failing records but also the data that is part of the failing records. Failing records That’s a very convenient one-liner! Below are other forms of generic tests shipped with dbt, for the accepted_values and relationships. The latter took to long to run and was cancelled midway. - name: end_station_name description: &quot;End Station Name&quot; tests: - relationships: to: ref(&#39;citi_trips_round&#39;) field: end_station_name - name: gender description: &quot;Gender (unknown, male, female)&quot; tests: - accepted_values: values: [&#39;unknown&#39;, &#39;male&#39;, &#39;female&#39;] It’s been quite an exciting journey with tests. And for sure dbt tests do grill your data! "],["dbt-expectations-package.html", "Chapter 10 dbt Expectations package 10.1 dbt-expectations installation 10.2 Types of dbt-expectations tests", " Chapter 10 dbt Expectations package What is dbt-expectations? dbt-expectations is an extension package for dbt which works much akin to the Great Expectations package for Python. It was intentionally designed to provide Great Expectations like features in dbt, but now from dbt itself rather than integrating Great Expectations (GE). Unless you’ve used GE, you may be wondering what this is in the first place, and its okay to feel lost. GE is much like tests in the previous chapter, it conducts quality tests on your data flagging those that deviate from the set assertions. I would put dbt-expectations and GE on the same plane and use an allegory to drive the point home; that of a car. When buying a car, there are some common checklist items, and others bespoke depending on your car model. For example, an ordinary car must have the following features (at least most of them): have four wheels have a driver’s seat have a gear (whether manual or automatic) have headlights have a windshield The above list can go on and on depending on your knowledge of cars. But your checklist can also contain some unique items, but must-have lists depending on your car make. For example, here is a checklist of the Volvo XC60 T6 (sorry, my bias!): 0.9l/100km fuel consumption Allowed emissions 22g/km (the less the better) Hybrid fuel type So if you go to a showrooms and the beautiful or handsome sales agent takes you to the Volvo XC60, you will be viewing it as you cross your checklist items. dbt-expectations and GE work in the same way. 10.1 dbt-expectations installation According to the documentation dbt-expectations will work for dbt versions 1.7x and higher. Let’s first pass this little test. dbt --version If you get your dbt-core version is above 1.7x, then you can proceed. If not, you need to update your dbt. You can do so using python -m pip install --upgrade dbt-core or if you want to be more specific, this will do: python -m pip install --upgrade dbt-core==0.19.0. Ours, at the moment of writing this book, was version 1.8.7. Therefore we have a clean bill of health to proceed. dbt-expectations isn’t installed in the same type and enter kind of means like we did for dbt-core and dbt big-query. Nevertheless, some code is written in some YAML files and from henceforth dbt recognises it. First create a packages.yml file in the same level as your dbt_project.yml file. You can do so by running this command: touch packages.yml On the packages.yml file, insert the following: packages: - package: calogica/dbt_expectations version: [&quot;&gt;=0.10.0&quot;, &quot;&lt;0.11.0&quot;] Apart from that, the dbt-date dependency must also be installed. This is because dbt-expectations references to it. However, this will be installed in the dbt_project.yml file rather than the packages.yml file. So inside the dbt_project.yml paste the following just before the materializations dictionary. vars: &#39;dbt_date:time_zone&#39;: &#39;Africa/Nairobi&#39; You may insert any valid timezone apart from the one specified above. Now run dbt deps to seal the deal by installing the dbt-expectations package. dbt deps Here is output showing the successful installation of the package in our environment, and dbt-date too. 19:31:10 Running with dbt=1.8.7 19:31:12 Updating lock file in file path: /home/sammigachuhi/dbt_book2/dbt_book/package-lock.yml 19:31:13 Installing calogica/dbt_expectations 19:31:44 Installed from version 0.10.4 19:31:44 Up to date! 19:31:44 Installing calogica/dbt_date 19:31:45 Installed from version 0.10.1 19:31:45 Up to date! 10.2 Types of dbt-expectations tests dbt-expectations comes with a plethora of tests’ functions which can be classified into the following categories. Table shape Missing values, unique values, and types Sets and ranges String matching Aggregate functions Multi-column Distributional functions We will perform one test in each category just to exemplify the potential of dbt-expectations. 10.2.1 Table shape 10.2.1.1 expect_table_row_count_to_equal_other_table Description: Expect the number of rows in a model match another model. We will expect the citi_trips_round and the citi_trips_minutes tables to have the same number of rows since their respective models used the same citi_bike_trips table. Therefore, the two tables should pass this test, or will they? Since we only want to concentrate on the models within the my_models directory, just run: dbt test --select models/my_models Here is the output. -- snip -- 07:57:38 2 of 8 FAIL 1 dbt_expectations_expect_table_row_count_to_equal_other_table_citi_trips_minutes_ref_citi_trips_round_ [FAIL 1 in 4.83s] 07:57:38 3 of 8 START test dbt_expectations_expect_table_row_count_to_equal_other_table_citi_trips_round_ref_citi_trips_minutes_ [RUN] 07:57:42 3 of 8 FAIL 1 dbt_expectations_expect_table_row_count_to_equal_other_table_citi_trips_round_ref_citi_trips_minutes_ [FAIL 1 in 4.53s] -- snip -- This leaves one puzzled, why? A close look at the model for citi_trips_round reveals the answer. This model was designed to only work on non-null rows, unlike the citi_trips_minutes which worked on all rows, null or not. Therefore the citi_trips_round had less rows and thus the generated error. In case your tests results seem incongruent, it is always good to recheck the models to refresh your memory, as we did here. In fact, dbt did a good job of generating an SQL to show us the error: 07:58:03 Failure in test dbt_expectations_expect_table_row_count_to_equal_other_table_citi_trips_minutes_ref_citi_trips_round_ (models/my_models/my_models.yml) 07:58:03 Got 1 result, configured to fail if != 0 07:58:03 07:58:03 compiled code at target/compiled/dbt_book/models/my_models/my_models.yml/dbt_expectations_expect_table__c00100dada30a31f15f90b9c1ba0b295.sql If you click on the destination of the SQL statement and copy the contents to the SQL query tab of BigQuery, you will see the difference in row count for the two tables. Row count difference 10.2.2 Missing values, unique values, and types 10.2.2.1 expect_column_values_to_not_be_null Description: Expect column values to not be null. This is a no-brainer kind of test. Can you guess which columns in any of our tables in the nyc_bikes dataset that should never be null? Here is a clue, station_id and station names, unless the biker teleports to or from somewhere! So on the my_models YAML file, insert the below test on the start_station_id, start_station_name, end_station_id and end_station_name fields. tests: - dbt_expectations.expect_column_values_to_not_be_null You will get some interesting results. some of the tests fail for the citi_trips_minutes model because of the many null rows in the table. However, none of this particular test fail for the citi_trips_round table; it has zero null rows. A caveat when using the dbt_expectations.expect_column_values_to_not_be_null, only add a colon : when specifying more optional parameters such as row_condition: \"id is not null\" # (Optional). Otherwise, leave it out. 10.2.3 Sets and Ranges 10.2.3.1 expect_column_values_to_be_in_set Description: Expect each column value to be in a given set. This test works best for where you are sure that a certain column will only accept certain values. A good example is the gender column. Here we insert the test in our citi_trips_minutes and citi_trips_round models. - name: gender description: &quot;Gender (unknown, male, female)&quot; tests: - dbt_expectations.expect_column_values_to_be_in_set: value_set: [&#39;unknown&#39;,&#39;male&#39;,&#39;female&#39;] If you run the above test for both the citi_trips_minutes and citi_trips_round models, the test will fail for the former? Why, because of the pesky null rows. However, to take the null rows into consideration and take them as accepted values in the citi_trips_minutes model only, we simply add an empty quotation marks, like so (''). Here is our modified test: value_set: ['', 'unknown','male','female']. The test will then pass for our citi_trips_minutes table. 10.2.4 String matching 10.2.4.1 expect_column_value_lengths_to_be_between Description: Expect column entries to be strings with length between a min_value value and a max_value value (inclusive). In one of our generic tests, called the long_characters.sql inside the tests/generic folder, we set out to alert ourselves of any tests above the 15 word threshold. Now of course, we got some results, so let’s use this as the upper limit of our station names. Additionally, we will add 1 as the minimum value to flag off those station names without a single character. The null values should be easy to catch with this. Because our citi_trips_minutes table has several null rows, we used some fine-grained creativity to take this into account also for the station names. So for both the start_station_name and the end_station_name, we inserted the following test: tests: - dbt_expectations.expect_column_values_to_not_be_null - dbt_expectations.expect_column_value_lengths_to_be_between: min_value: 0 The minimum character length for a station name is 0, thus even null values will be accepted. However, we were more brutal for the citi_trips_round model, but used a high maximum character of length to be lenient to those stations very long names. tests: - dbt_expectations.expect_column_values_to_not_be_null - dbt_expectations.expect_column_value_lengths_to_be_between: min_value: 1 # (Optional) max_value: 70 # (Optional) We are glad to know both models passed this simple test. 10.2.5 Aggregate 10.2.5.1 expect_column_max_to_be_between Description: Expect the column max to be between a min and max value You may wonder what the purpose of this test is. But don’t dismiss it yet, it can come quite in handy when searching for outlier values. We will demonstrate it in catching overly long bike trips. However, this test needs some background knowledge of your data. Applying the below queries on BigQuery will help. SELECT AVG(trip_duration_min) FROM dbt-project-437116.nyc_bikes.citi_trips_minutes; -- 16 SELECT MAX(trip_duration_min) FROM dbt-project-437116.nyc_bikes.citi_trips_minutes; -- 325167.48 SELECT * FROM dbt-project-437116.nyc_bikes.citi_trips_minutes WHERE trip_duration_min &gt; 200000; Now lets place the limits of our maximum trip duration values to be between the average of 16 and some intermediate value such as 100, 000 minutes (1, 666 hours)! tests: - dbt_expectations.expect_column_max_to_be_between: min_value: 16 # (Optional) max_value: 100000 # (Optional) That will flag off some errors, but if you change the max_value parameter to 360000, the tests will pass. We used some fine-grained creativity in this case where all trip_duration_min fields in both the citi_trips_minutes and citi_trips_round had 360000 as their max_value parameter. However, in the trip_min_round field of the citi_trips_round model, we set the max_value as 100000 to demonstrate the error. 10.2.6 Multi-column 10.2.6.1 expect_column_pair_values_A_to_be_greater_than_B Description: Expect values in column A to be greater than column B. This kind of test comes in handy when you want to ensure that one of your columnar values is greater than, or less than that of a different column. A good example is a comparison of trip duration in seconds in the tripduration column versus the trip duration in minutes from trip_min_round column in our citi_trips_round table. Surely time in seconds will always have a greater value in terms of length than the more concise minutes values! In our citi_trips_round model, insert the test as follows: - name: citi_trips_round description: &#39;{{ doc(&quot;citi_trips_round&quot;) }}&#39; tests: - dbt_expectations.expect_table_row_count_to_equal_other_table: compare_model: ref(&quot;citi_trips_minutes&quot;) - dbt_expectations.expect_column_pair_values_A_to_be_greater_than_B: column_A: tripduration column_B: trip_min_round It surely does pass the test. -- snip -- 09:40:46 5 of 26 PASS dbt_expectations_expect_column_pair_values_A_to_be_greater_than_B_citi_trips_round_tripduration__trip_min_round [PASS in 2.20s] 10.2.7 Distributional functions This is another category of shipped-in tests of dbt-expectations. However, they require some statistical homework to be conducted on your data prior to applying the tests. The tests under this category include: expect_column_values_to_be_within_n_moving_stdevs, expect_column_values_to_be_within_n_stdevs and expect_row_values_to_have_data_for_every_n_datepart. "],["seeds.html", "Chapter 11 Seeds 11.1 Uploading a seed into your data warehouse 11.2 Referencing seeds in models", " Chapter 11 Seeds Seeds are Comma Separated Values (csv) files stored inside your seeds directory which can be loaded into your data warehouse using the dbt seed command. Seeds can also be referenced by your SQL models using the ref() function. Seeds in dbt are version controlled, that is, you can revert them to a previous state. Seeds are best used for data that changes infrequently. A good example is country codes, email accounts and station names. However, seeds should not be used to store sensitive information such as passwords. To demonstrate about seeds in dbt, we will try to upload a New York City (NYC) bike history data for 2014. Extract the zip folder and copy the 3e7acf34-19ba-4bf4-8dd2-cf349623dc6b.csv inside the dbt_book/seeds directory. To avoid being verbose, let’s rename it to 2014-tripdata.csv. 11.1 Uploading a seed into your data warehouse Believe you me we had a better csv table to upload, one much more related to the NYC bikes dataset. However, because it was ~320MB and we want to be economical in upload times, we settled for this historical data. Nevertheless, keeping on with this chapter, to upload a seed into a data warehouse, we use this command: dbt seed After that, it’s a test of patience. Depending on your upload speeds, it shouldn’t take long to upload a 36MB file to BigQuery. -- snip -- 16:31:51 Concurrency: 1 threads (target=&#39;dev&#39;) 16:31:51 16:31:51 1 of 1 START seed file nyc_bikes.2014-tripdata ................................. [RUN] 16:33:17 1 of 1 OK loaded seed file nyc_bikes.2014-tripdata ............................. [INSERT 224736 in 85.39s] 16:33:17 16:33:17 Finished running 1 seed in 0 hours 1 minutes and 28.54 seconds (88.54s). 16:33:17 16:33:17 Completed successfully 16:33:17 16:33:17 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 If you go to BigQuery and refresh the contents of your nyc_bikes dataset, you should see the 2014-tripdata table present. Seeds 11.2 Referencing seeds in models Just like you would reference a model in another model, we can also reference seeds in another model. All your need to reference a seed is to place the name of the csv file, excluding the .csv extension inside the ref() function. For example, we want to create a view that contains those start station names from our 2014 table that are existent in the citi_trips_long model. Within the models/my_models directory, create the citi_stations_2014.sql model with the following query: {{ config(materialized=&#39;view&#39;) }} WITH citi_stations_2014 AS ( SELECT * FROM {{ref (&#39;2014-tripdata&#39;) }} WHERE `start station name` IN ( SELECT start_station_name FROM {{ ref(&#39;citi_trips_long&#39;) }} ) ) SELECT * FROM citi_stations_2014 Thereafter, run the model using dbt run --select citi_stations_2014 That will create a view that contains only those stations within the 2014-tripdata.csv also within the citi_trips_long model. Much to our surprise, all the stations within our 2014 table are also found in the citi_trips_long model! Here are the SQL queries we used to perform a count of each of the two tables in BigQuery. SELECT COUNT(*) FROM `dbt-project-437116`.`nyc_bikes`.`2014-tripdata`; SELECT COUNT(*) FROM dbt-project-437116.nyc_bikes.citi_stations_2014; Both returned a value of 224736. Here is the view of the citi_stations_2014 model in BigQuery. View from seed "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
