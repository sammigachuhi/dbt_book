[["index.html", "dbt Book Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " dbt Book Samuel Gachuhi Ngugi 2024-10-02 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["introduction.html", "Chapter 2 Introduction 2.1 What is dbt? 2.2 Encounter with dbt 2.3 dbt, from the professionals… 2.4 Why use dbt?", " Chapter 2 Introduction 2.1 What is dbt? dbt, when in full, stands for Data build tool. dbt is a tool that data scientists and analytical engineers use to transform data in their data warehouses. If you are a newbie wanting to, or rather curious about dbt, the preceding statements sure contains a lot. The words analytical engineers, transform data and data warehouses may sound unfamiliar, if not imposing. For now just think of dbt much like a recipe. In a recipe, you have the steps and the instructions to cook your favourite meal, say a roasted chicken. Sure enough, your recipe will contain details on the optimal oven temperature, heating duration and how to set it! dbt works in much the same way. We define how we want to transform or build our data. Once we hit run the magic happens. 2.2 Encounter with dbt How did I come across dbt? Being from a totally different background, the geographical sciences, my first experience with dbt, contrary to what veteran users may say, wasn’t so good. Either because a lot was on my desk back then, but I was having trouble piecing together all the different components that make dbt work, or dbt uses to work. Whichever is the case. It is only after some time, and several hard knocks in between, that I was able to get a semblance of what it does. At least I got a few things. dbt could be used to create views of your tables in the data warehouse, it could perform tests and lastly, (the one I liked the most) it could be used to render a website of your documentation! 2.3 dbt, from the professionals… dbt, from the words of developers, is an open-source tool that analysts and data engineers use to transform data in their data warehouses. Did someone mention transform data somewhere? Data transformation is the process of converting data from its source format to the format required for analysis. The data transformation process is part of a three stage process known as Extract Load and Transform (ELT). Before ELT, Extract Transform Load was the king. The former involves transfering data from the source, to the destination, such as a data warehouse or data lake and performing the transformation in the destination. The latter, though a traditional approach, involves first identifying the data, transforming it prior to landing it to the destination, in this case data warehouse and letting it rest there where downstream users can get hold of it. Here is a better description of the Extract, Load and Transform keywords. Extract - this is the identification and reading of data from one or more sources, such as databases, internet, comma separated value (csv) files and the like. Load - just like you would pull up a weight into a lorry, this is the process of transferring data from the source to your data warehouse. Transform - this is the conversion of data from its state to a format that can be used by downstream users. You may have seen the term data warehouse coming up quite a number of times. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. Examples of data warehouses are Google Big Query, Snowflake, Amazon Redshift, Azure Synapse Analytics, IBM Db2 Warehouse and Firebolt. 2.4 Why use dbt? If you work with data that needs to be version controlled, that is, it can be rolled back to a previous time, you need to work in dbt. If you want to standardize the data models created across teams, dbt is the tool of choice. If you also want a central place where your data work is documented, dbt handles this quite well. In other words, dbt should be the swiss knife when working with large datasets and you want to maintain modularity, order and documentation of your work. The below image summarises the role of dbt in your data processing work. The role of dbt Source: Reference "],["the-dbt-architecture.html", "Chapter 3 The dbt architecture", " Chapter 3 The dbt architecture In my first time working with dbt, I was overwhelmed with its architecture. Similarly to Django, if one didn’t understand a particular component, it would be enough to break your app. Same case with dbt. Sometimes it feels like that individual who is sitting before that large screen in a nuclear power plant and in charge of all the controls. Sometimes it feels like that, like you have to be on top of everything in dbt, like the chap in that nuclear power plant who has to be on top of all the dials and valves. The main components that make up dbt, and which are used in most cases are: models tests documentation sources Let’s go through each one. 3.0.1 Models This is the component of dbt that you will most likely work with. In dbt, a model is simply a SQL statement. As simple as that. dbt will use the SQL statements to perform the transformations in your data warehouse that have been defined in your SQL statement. For example, say I want to create a new column of the table in my Google BigQuery. I will create a SQL statement that does just that. That SQL statement is what is referred to as a model in dbt. Below is an example of a model that creates a table called customers. The model is saved as customers.sql. with customer_orders as ( select customer_id, min(order_date) as first_order_date, max(order_date) as most_recent_order_date, count(order_id) as number_of_orders from jaffle_shop.orders group by 1 ) select customers.customer_id, customers.first_name, customers.last_name, customer_orders.first_order_date, customer_orders.most_recent_order_date, coalesce(customer_orders.number_of_orders, 0) as number_of_orders from jaffle_shop.customers left join customer_orders using (customer_id) 3.0.2 Tests “Do not put me to test”, is a familiar statement we have heard from an already impatient person. However, dbt allows us to test our data and see if it meets certain assertions. In other words, does our data meet the requirements that have been set for it? dbt offers two ways to perform your tests: 1) generic and, 2) custom tests. Generic tests involve just using a pre-defined test that comes packaged in dbt. For example, for every field key you place in a yml file in dbt, you can specify which kind of test to perform on that particular field from the following options: unique, not_null, accepted_values and relationships. unique - the values should be radically distinctive all through not_null - there shouldn’t be a missing value in the particular column name in the table accepted_values - only the values contained in the accepted values key will be considered valid. Anything outside of this will result in an error relationships - the values in this field can be referenced in a different column elsewhere in the table or on a different table altogether. An example of a generic test is below: version: 2 models: - name: orders columns: - name: order_id tests: - unique - not_null - name: status tests: - accepted_values: values: [&#39;placed&#39;, &#39;shipped&#39;, &#39;completed&#39;, &#39;returned&#39;] - name: customer_id tests: - relationships: to: ref(&#39;customers&#39;) field: id For custom tests, these involve one creating a SQL model and referencing it in a yml file using Jinja template language. For example, here is a custom test written a SQL file called transaction_limit_test.sql. -- tests/transaction_limit_test.sql select user_id, sum(transaction_amount) as total_spent from {{ ref(&#39;transactions&#39;) }} group by user_id having total_spent &gt; 10000 -- Assuming the limit is 10,000 The test is referenced in a yml file and called over a column called transactions. models: - name: transactions tests: - transaction_limit_test 3.0.3 Documentation Now the favourite part of dbt, and possibly the easiest is documentation. Documentation is the description of various components of your data. To write a description of any piece of your data, the description key is used. For example here is a description of a field called event_id inside a yml file. version: 2 models: - name: events description: This table contains clickstream events from the marketing website columns: - name: event_id description: The D-day is the Deed day tests: - unique - not_null Documentation will be performed where you have placed your tests. There is also a more complex, but scalable manner of writing descriptions. It uses jinja template tags. It works well for large data where the descriptions are many or the descriptions are shared across several tables. A short example of the jinja templates’ documentation is this. I will write the description in a different file, a markdown file (.md) for the matter, other than the one containing my field names. The descriptions will be like so: {% docs table_events %} I am not so very robust, but I’ll do the best I can. Some text here 1) and here 2) and here 3) and also here {% enddocs %} So when one returns to their yml file, they will reference the particular field of interest with the above description like so: version: 2 models: - name: events description: &#39;{{ doc(&quot;table_events&quot;) }}&#39; columns: - name: event_id description: The D-day is the Deed day tests: - unique - not_null 3.0.4 Sources sources enable one query the data in your datawarehouse. Once you specify the existing table in your datawarehouse under the sources key, you can access every data from within this table using SQL. To work with a source table, you first have to wrap it inside a {{ source(table-name) }} jinja template. Below is an example of declaring a source. version: 2 sources: - name: jaffle_shop database: raw schema: jaffle_shop tables: - name: orders - name: customers - name: stripe tables: - name: payments You can reference the above source inside a SQL model like so: select ... from {{ source(&#39;jaffle_shop&#39;, &#39;orders&#39;) }} left join {{ source(&#39;jaffle_shop&#39;, &#39;customers&#39;) }} using (customer_id) dbt will thereafter know that it will perform some operations using data from the orders and customers data from the jaffle_shop –the origin of all our data in this example. "],["data-storage.html", "Chapter 4 Data storage 4.1 Data warehouse 4.2 Data lake 4.3 Data lakehouse 4.4 A brief history", " Chapter 4 Data storage As as been repeatedly mentioned, to the point of boredom, dbt transforms the data in your data warehouse. Now, before expanding the concept of a data warehouse, the following two are also terms you will here mentioned quite often in the field of analytical engineering. They are data lake and data lakehouse. 4.1 Data warehouse At the very beginning, when introduced to data engineering concepts with a test paper to boot in four weeks time, I thought that a data warehouse was some storage system akin to that found in Google Drive. I could have been partly right, but I was still far off the mark. A data warehouse is more than just a storage system. It is where data is not only stored but also queried, by means of SQL. It allows data from multiple sources such as internet of things, apps, from emails to social media and keeps a historical record of any changes. Examples of data warehouses are Snowflake, Google Big Query, Amazon Redshift and Azure Synapse Analytics. The following are the components of a data warehouse. Data sources - this refers to the origins of the data that lands in your data warehouse. Extract, Transform and Load (ETL) Processes - these are the processes involved in extracting, transforming and loading the data into your data warehouse. Data warehouse database - this is the central repository where the cleansed, integrate and historical data is stored. Metadata repository - metadata is essentially data about data. Metadata will typically contain the source, usage, values and other features that comprise your data. Access tools - imagine having to figure a way how to write a document in your computer without Microsoft Word. How hard would that be? Access tools are similar to Microsoft Word in the aformentioned allegory. These are the tools that enable a user to interact with the data. They include querying, reporting and visualization tools. As you can see from above, a data warehouse is more than just a storage area for your data. It is like a whole community that will provide the services that you desire, so long as they are present in the data warehouse. Source: Reference 4.2 Data lake A data lake is a centralized repository that ingests and stores large volumes of data in its original form. Due to its open, scalable architecture, a data lake can store structured (database tables, excel sheets), semi-structured (xml, json and web pages) and unstructured data (images, audio, tweets) all in one place. Data in the data lake is stored in its original format. So if data lakes and data warehouses store data, then what is the difference? For one, a data lake can store data of any type, so long as it falls within the three classes of structured, semi-structured and unstructured. On the other hand, data warehouses deal with more standardized data. That is, data in a data warehouse has undergone some refinement of some kind to be in a structure that fits the organizations’s goals. Source: Reference 4.3 Data lakehouse A data lakehouse is simply a hybrid of both a data warehouse and data lake. It is like a product that combines the best from both worlds. This is what a data lakehouse provides the following characteristics: scalability of large sums of data from the data lake and the application of a structural schema to data as seen in data warehouses. Even with the above definition, it is still hard to decipher the advantage that a data lakehouse offers above that of a data warehouse. Apart from allowing the querying of unstructured data, storage costs are lower in a data lakehouse compared to a data warehouse. Data lakehouse Source: Reference 4.4 A brief history At the very beginning, companies used to rely on relational databases and these were sufficient. However, they became too numerous as the needs and services of companies grew. Therefore, experts decided to look for a way of how they could merge all these single databases into one repository which would hold everything while allowing for permission controls to who gets access to what. Believe it or not, the concept of the data warehouse began in the 1960s but in the 1980s and 1990s is when it was hot. That’s until the need for storing unstructured data from emails, images and audio began to grow and data warehouses were not so efficient in storing this thanks to… their strict schema enforcement (read, they store data in a structured format). The beginning of 2000s saw the rise in the need to properly manage unstructured data with the growth of online platforms such as Google and Yahoo. Companies needed a way to store and retrieve unstructured data quickly and efficiently, which wasn’t possible with data warehouses. Data lakes excelled in storing all sorts of data, from structured to unstructured and everything in between in their raw format. If you read on the history of data lakes, you will come across the word ‘Hadoop’ quite often. This was the pioneer of the data lakes we use today. However, despite being a good storage for any sort of data, the pesky question of maintaining some quality and order resurfaced again! How could we maintain some structure while allowing the data to be in any structure?! From 2010s and onwards, after a decade of success with data lakes, companies wanted a better storage system from which to run their machine learning models but had the best capabilities of both a data warehouse and a data lake. Before lakehouses, companies would first ingest data into a data lake, then load into a data warehouse from where analytics would be done. But how could we just merge it into one place where storage and analytics could happen? This is how the data lakehouse concept came to be. Data lakehouses provided the following benefits: ACID (Atomicity, Consistency, Isolation and Durability) transactions. ACID transactions promote integrity during data transfer. Delta lake - initially developed by the Databricks team, this is a layer on top of your data in the data lake that provides a schema, keeps a record of changes in your data (versioning) and stores metadata. Machine learning support - because a data lakehouse can store more data types than the data warehouse, it is a better place to perform machine learning modeling. For more information on the evolution of data storage systems, this is a definitive guide. Source: Reference "],["our-data-in-bigquery.html", "Chapter 5 Our data in BigQuery 5.1 Accessing Big Query 5.2 Copying the New York City Bikes data", " Chapter 5 Our data in BigQuery In an earlier chapter, we saw that in data engineering data mainly goes through three processes: extract, load and transform (ELT). The Extract, Transform, Load (ELT) is more of a traditional approach and we will not use it in this case. We will be using Google Bigquery as our data warehouse when working with dbt. As a reminder, let’s go through the definitions of ELT. Extract - the process of identifying and reading data from one or more source systems. We won’t have to do this since the New York City (NYC) bikes data that will be using has already been extracted from whichever the source is by the trusty workers of Google. Load - the process of adding the extracted data to the data warehouse, –in this case Google BigQuery. Again, Google has done this for us. Therefore we won’t have to do it. Transform - the process of converting data from its raw format to the format that it will be use for analysis. This falls definitely within our forte. And we shall use dbt for this. Examples of data transformations that can be done with SQL modeling in dbt are: Replacing codes with values Aggregating numerical sums Applying mathematical functions (SQL can do some maths too, but can be very verbose here) Converting data types Modifying text strings Combining data from different tables and databases. 5.1 Accessing Big Query BigQuery is a data warehouse provided by Google. To access it, open an incognito window and go to this link. Sign in using your gmail account. Click on Console button at the top right. That step of bravery will take you to an interface that looks like this: GCP Interface Click on the dropdown at the top. Select NEW PROJECT. We want to create a new project that will contain some tables that we will work with in dbt. Name your project as dbt-project1 or any other name you prefer. Then select CREATE. Once the project has been created, you will be returned to the original page as first. However, when you select the project dropdown again, you should see your newly created project as one of the options. GCP Project Click on your project, the interface will refresh and the dropdown should now reflect dbt-project1. Click on the Dashboard link on the homepage. The below interface should appear. It can seem overwhelming at first. Dashboard In one of the “boxes” within the Dashboard tab, you will find one called Resources with the BigQuery button underneath. Click on this button. It will take you to a page asking you to Enable the BigQuery Application Programming Interface (API). Kindly comply! Behold, below is the BigQuery interface. BigQuery interface You will see one of the resources as dbt_project1&lt;some-random-number&gt; in case you had other resources. Star this project for quick access in future. 5.2 Copying the New York City Bikes data One of the datasets we will be working with is the “New York City Bikes dataset”. To access it, click on the ADD button. A sidebar will open up. Go to Public Datasets. In the Search Marketplace searchbar, type ‘bikes’. Marketplace Click on the NYC Citi Bike Trips tab. A new sidebar will popup with a button of View Dataset. Click this button and the Google Cloud Platform (GCP) Dashboard will reappear but this time round the bigquery-public-data resource will appear. Click on this particular resource’s dropdown on the left and scroll down to the new_york_citibike dataset. We want to copy this dataset from that of bigquery-public-data to that of dbt_project1-437718. The random numbers will be different in your case. Scroll up again to your dbt_project1 resource. On the kebab menu on the right of this resource, select Create Dataset. Create dataset A new sidebar will open. Insert the following for each parameter: Dataset ID - nyc_bikes Location type - Region Region - africa-south1 (Johannesburg) or your preferred region Thereafter, click on CREATE DATASET. The nyc_bikes dataset should now appear under the dbt-projec1 resource. We want to copy the contents of the new_york_citibike dataset into our nyc_bikes dataset. So how do we proceed? Scroll down to the new_york_citibike dataset under the bigquery-public-data resource and click on it. On the menu for this dataset, you will see the Copy button. Click on this button. Copy dataset Copy sidebar In the Destination searchbar, type nyc_bikes in reference to where we want to copy the contents into. You may need to enable the data transfer API to perform the copy operation. Do so if BigQuery necessitates you that it must be enabled. Once you copy the dataset, a small bar will appear on the screen saying View Details. Click on it to stop the run operation since BigQuery will be rerunning the copy operation after every 24 hours. Disable the transfer process and delete it. Going back to your dbt_project1 resource, your nyc_bikes dataset should now be having two tables under it. That is: citibike_stations citibike_trips Click on any of the tables and preview the data therein using the PREVIEW button of each tables interface. Table Congratulations on loading your first table in BigQuery! "],["installing-dbt.html", "Chapter 6 Installing dbt 6.1 Setting the environment 6.2 Connecting to your BigQuery data warehouse 6.3 Initializing a dbt project", " Chapter 6 Installing dbt Now that we’ve seen how to access a dataset inside our data warehouse, now let’s proceed to installing dbt. This section assumes you have Visual Studio (VS) Code already installed. All code in this book has been done inside a Linux environment but on a Windows computer thanks to the Window Subsystem for Linux (WSL2) virtualization platform. 6.1 Setting the environment Open your VS Code. Create a new folder called dbt_book. Move into this directory in your VS Code by typing cd dbt_book/ in your terminal. The first thing we shall do is create a virtual environment from which we shall conduct all our dbt operations. A virtual environment is useful in preventing conflicts between packages across your various programming projects. python3 -m venv venv The first venv tells python that you’re creating a virtual environment while the second refers to the name of the virtual environment. In this case, our virtual environment shall still share the name venv. Now let’s activate our virtual environment. source venv/bin/activate You will see your namespace appended with venv which means that your virtual environment is now active. For example: (venv) sammigachuhi@Gachuhi:~/dbt_book$ Now here comes the big part: installing dbt for Big Query. We just don’t want to install dbt-core, the dbt packages that we’ll be using, but we also want to install the necessary dependencies that will connect it to BigQuery, where our data is stored. The following code will install everything we need; both dbt and the dependencies needed to connect it to BigQuery. python3 -m pip install dbt-core dbt-bigquery 6.2 Connecting to your BigQuery data warehouse We wish connecting to a data warehouse for dbt were as easy as providing a username and password. However, it is not so. But it is definitely possible. To connect dbt to a data warehouse, we use a keyfile. A keyfile is a file that contains encryption keys or licenses for a particular task. The keyfile we shall use shall be the doorway to our data warehouse. First step, go to your GCP Credentials Wizard page. Ensure that your project is set to the dbt project you created in the previous chapter. For my case, I reverted to an earlier created project called dbt_project since my other project dbt_project1 started incuring costs. For Credential Type: From the Select an API dropdown, choose BigQuery API Select Application data for the type of data you will be accessing Click Next to create a new service account. In the service account page: Type dbt-book as the Service account name or any other name you prefer. From the Select a role dropdown, choose BigQuery Job User and BigQuery Data Editor roles and click Continue Leave the Grant users access to this service account fields blank Once everything is fine it is as good as clicking Done! Your credentials interface will look like below. Service account Click on your service account name. Click on the KEYS tab. We want to create a key that dbt will use to connect to our data warehouse. Click on ADD KEY&gt;Create new key. Add key Select JSON on the interface that appears and click CREATE. This will download a json file containing the encryption keys that dbt will use to connect to your data warehouse. Store this json file in a safe place. 6.3 Initializing a dbt project To create a dbt project, we run the open sesame key: dbt init. It will create a string of outputs. It’s important to key in the right details if you want to create a dbt project. The first output will ask for the name of your dbt project. Insert dbt_book or any other name you prefer. 19:07:31 Running with dbt=1.8.7 Enter a name for your project (letters, digits, underscore): dbt_book If you had an already pre-existing dbt project with the same name, dbt will ask if it can overwrite that project. Type y if you wish to do so. dbt will thereafter ask you which database you would like to use. Since we had installed dbt with the package dependancies for BigQuery, you will see the sole option for BigQuery. Type 1 to select BigQuery. Which database would you like to use? [1] bigquery (Don&#39;t see the one you want? https://docs.getdbt.com/docs/available-adapters) Enter a number: 1 You will thereafter be asked the authentication method you would like to use. Since we had already created a service account and downloaded the JSON file containing the encryption keys, we shall select option 2. Enter a number: 1 [1] oauth [2] service_account Desired authentication method option (enter a number): 2 For the keyfile, provide the path to where you had saved the json file. As a note, this path should be somewhere different than where your dbt project is located. This is because saving the project into Github with your json keys as part of its files will cause Github to raise an alarm and send consistent emails. This is because the json keys are never meant to be shared or stored somewhere accessible. They are considered sensitive information keyfile (/path/to/bigquery/keyfile.json): /home/sammigachuhi/dbt_credentials/dbt_book.json You will also be asked to provide your project ID. This is available under your dbt project’s dashboard under the Project ID heading. project (GCP project id): dbt-project-437116 For the dataset name, we will use nyc_bikes which is the dataset we want to conduct our dbt operations on. dataset (the name of your dbt dataset): nyc_bikes For the rest of the options, you can fill them as below: threads (1 or more): 1 job_execution_timeout_seconds [300]: [1] US [2] EU Desired location option (enter a number): 1 19:09:24 Profile dbt_book written to /home/sammigachuhi/.dbt/profiles.yml using target&#39;s profile_template.yml and your supplied values. Run &#39;dbt debug&#39; to validate the connection. Now, in order to test whether your dbt installation is correct, you will have to change directory into your dbt_book subfolder we created as part of the dbt init prompts. At first, we had created a directory called dbt_book in which we also activated the virtual environment. When we ran dbt init from this directory, we specified our project name to be dbt_book as well. It is from here we want to check if our dbt initialization and access to BigQuery was successful. So move into this subfolder via cd dbt_book/. (venv) sammigachuhi@Gachuhi:~/dbt_book$ cd dbt_book/ (venv) sammigachuhi@Gachuhi:~/dbt_book/dbt_book$ dbt debug Inside the dbt_book subfolder we created as part of the dbt initialization prompts, run dbt debug. If the final output of the run is All checks passed!, you are good to go! 19:10:20 Running with dbt=1.8.7 19:10:20 dbt version: 1.8.7 19:10:20 python version: 3.10.12 19:10:20 python path: /home/sammigachuhi/dbt_book/venv/bin/python3 19:10:20 os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 19:10:21 Using profiles dir at /home/sammigachuhi/.dbt 19:10:21 Using profiles.yml file at /home/sammigachuhi/.dbt/profiles.yml 19:10:21 Using dbt_project.yml file at /home/sammigachuhi/dbt_book/dbt_book/dbt_project.yml 19:10:21 adapter type: bigquery 19:10:21 adapter version: 1.8.2 19:10:22 Configuration: 19:10:22 profiles.yml file [OK found and valid] 19:10:22 dbt_project.yml file [OK found and valid] 19:10:22 Required dependencies: 19:10:22 - git [OK found] 19:10:22 Connection: 19:10:22 method: service-account 19:10:22 database: dbt-project-437116 19:10:22 execution_project: dbt-project-437116 19:10:22 schema: nyc_bikes 19:10:22 location: US 19:10:22 priority: interactive 19:10:22 maximum_bytes_billed: None 19:10:22 impersonate_service_account: None 19:10:22 job_retry_deadline_seconds: None 19:10:22 job_retries: 1 19:10:22 job_creation_timeout_seconds: None 19:10:22 job_execution_timeout_seconds: 300 19:10:22 timeout_seconds: 300 19:10:22 client_id: None 19:10:22 token_uri: None 19:10:22 dataproc_region: None 19:10:22 dataproc_cluster_name: None 19:10:22 gcs_bucket: None 19:10:22 dataproc_batch: None 19:10:22 Registered adapter: bigquery=1.8.2 19:10:26 Connection test: [OK connection ok] 19:10:26 All checks passed! "],["models-1.html", "Chapter 7 Models 7.1 Running a model 7.2 Model structure 7.3 A custom model", " Chapter 7 Models If you have gone through previous chapters, you will by now know that a model in dbt is any SQL file. It is what dbt will use to build tables, views and any other transformations in your data warehouse (read BigQuery). In dbt, models are executed with the hit and run command: dbt run. 7.1 Running a model dbt did us a very big favour during installation, it came with two models already created for us. These are namely the my_first_dbt_model.sql and my_second_dbt_model.sql within the models/example directory. It also provided a schema.yml file within the same directory which provides definitions for the model’s schema. Alright. Assuming that you are within the dbt_book subdirectory and your virtual environment (venv) already activated, type the following in your terminal dbt run. (venv) sammigachuhi@Gachuhi:~/dbt_book/dbt_book$ dbt run This will initiate a series of printouts. However, before we go to the expected output, you may run into an error related to the location not being found. 404 Not found: Dataset dbt-project-437116:nyc_bikes was not found in location US; reason: notFound, message: Not found: Dataset dbt-project-437116:nyc_bikes was not found in location US When we were initializing our project using dbt init we selected option 1 for US since there was no other option apart from EU. Luckily, there is a work around to this. It involves editing the projects.yml file. If you run dbt debug it will also show the path of your projects.yml alongside other configuration information. 18:19:56 Running with dbt=1.8.7 18:19:56 dbt version: 1.8.7 18:19:56 python version: 3.10.12 18:19:56 python path: /home/sammigachuhi/dbt_book/venv/bin/python3 18:19:56 os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 18:19:57 Using profiles dir at /home/sammigachuhi/.dbt 18:19:57 Using profiles.yml file at /home/sammigachuhi/.dbt/profiles.yml --snip-- Go to the provided path for profiles.yml which is found at /home/sammigachuhi/.dbt in my case. Open it and change the line with location to read from US to africa-south1. dbt_book: outputs: dev: dataset: nyc_bikes job_execution_timeout_seconds: 300 job_retries: 1 keyfile: /home/sammigachuhi/dbt_credentials/dbt_book.json location: africa-south1 --snip--- Now come back, and rerun dbt run again. dbt should now be able to run against your warehouse and create a table called my_first_dbt_model and a view my_second_dbt_model in BigQuery. “How do I know that my queries ran successfully?”, you may ask. One, the output of a successful dbt run is Completed successfully. Beneath this, message, will be a log of the number of models ran and if there have been any errors. We had two models, so we expect to see this reflect in the log. And it did. 18:21:16 Running with dbt=1.8.7 18:21:17 Registered adapter: bigquery=1.8.2 18:21:17 Unable to do partial parsing because saved manifest not found. Starting full parse. 18:21:19 Found 2 models, 4 data tests, 479 macros 18:21:19 18:21:22 Concurrency: 1 threads (target=&#39;dev&#39;) 18:21:22 18:21:22 1 of 2 START sql table model nyc_bikes.my_first_dbt_model ...................... [RUN] 18:21:30 1 of 2 OK created sql table model nyc_bikes.my_first_dbt_model ................. [CREATE TABLE (2.0 rows, 0 processed) in 7.83s] 18:21:30 2 of 2 START sql view model nyc_bikes.my_second_dbt_model ...................... [RUN] 18:21:34 2 of 2 OK created sql view model nyc_bikes.my_second_dbt_model ................. [CREATE VIEW (0 processed) in 4.01s] 18:21:34 18:21:34 Finished running 1 table model, 1 view model in 0 hours 0 minutes and 14.88 seconds (14.88s). 18:21:34 18:21:34 Completed successfully 18:21:34 18:21:34 Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2 The second way, and the most obvious, is checking the results in your BigQuery data warehouse. If you see the table and view my_first_dbt_model and my_second_dbt_model respectively, the query ran successfully. Models It was that simple, isn’t it? 7.2 Model structure Let’s take a look at the model my_first_dbt_model.sql. {{ config(materialized=&#39;table&#39;) }} with source_data as ( select 1 as id union all select null as id ) select * from source_data /* Uncomment the line below to remove records with null `id` values */ -- where id is not null The line {{ config(materialized='table') }} tells dbt to make the output my_first_dbt_model as a table. Any configurations set at the model level will overide the overarching ones set at dbt_project.yml file. If you quickly take a sneek peek at the dbt_project.yml file and scroll to the bottom, you will see this configuration: models: dbt_book: # Config indicated by + and applies to all files under models/example/ example: +materialized: view This configuration simply says that for every model inside the dbt_book/example directory, as displayed by the new line and identations, materialize the result as a view1. However, inside our my_first_dbt_model.sql file, we set the materialization as table. What is in the sql model will override what is in the dbt_project.yml. However, for my_second_dbt_model.sql, we didn’t specify the materialization. Like the case of the rat reigning in the cat’s absence, the materialization specified in the dbt_project.yml will call the shots for the second model. That’s why for my_second_dbt_model the materialization is a view, and not a table as its counterpart. Materialization is the persistence of a model in the data warehouse. The second part of our first model is the actual SQL statement. with source_data as ( select 1 as id union all select null as id ) select * from source_data This is a WITH SQL statement. In very simple terms, the SQL statement in parentheses () is what is referenced as source_data. Once we define our SQL statement and close it with parentheses, we can now select the data referenced by source_data. The SQL statements in parentheses is what is referred to as Common Table Expression (CTE). They were designed to simplify complex queries and be used in the context of a larger query. The second model doesn’t have much to provide but it introduces a new trick: the ref function. select * from {{ ref(&#39;my_first_dbt_model&#39;) }} where id = 1 The ref function is part of the jinja templating language. It is used to reference a model that has been provided within the parentheses enclosed with quotes (''). Therefore, in essence, our model is simply returning the row(s) from my_first_dbt_model that contain the value 1 in the id column. Here is the result of my_second_dbt_model view when I query BigQuery to show its contents. Second model 7.3 A custom model Now that we have seen how to run our models, and we have hit the ground running with some of the models preshipped with dbt, it’s now time to fold our shirts and run our own custom model. Let’s start easy. We will be no means delve into a complex model since that is only limited by your imagination. If you know SQL, there is no limit to the models, both in number and complexity, that you can do in dbt. Our first model will endeavour to perform a change on the citi_trips table within our nyc_bikes dataset. There is a very ripe column called tripduration which stands for the trip in second every cab ride took. Humans prefer to work in minutes so a trivial dbt job would involve creating a column that shows the tripduration in minutes. dbt would then be doing the Transform part in the ELT. In the citi_trips_minutes.sql model, we have written a code that creates a view of this result. Remember, views are just virtual tables but they do save on storage! {{ config(materialized=&#39;view&#39;) }} WITH citi_trips AS ( SELECT *, tripduration / 60 AS trip_duration_min FROM `dbt-project-437116.nyc_bikes.citibike_trips` ) SELECT * FROM citi_trips The above SQL statement is also a form of Common Table Expression (CTE). It’s not that hard. The variable citi_trips just references the SQL statement in parentheses. There is another small trick of the trade. What if this very simple dbt model was just part of thousands, and longer running SQL models. If we wanted to just run this small, tiny winy model, would it have to be part of the entourage of the other model runs. Such would be a waste of time and resources, assuming some of them are heavy models. No. We can use the --select keyword to only select the model we want to run. Below we run the citi_trips_minutes model. dbt run --select models/example/citi_trips_minutes.sql Below is the output. Concurrency: 1 threads (target=&#39;dev&#39;) 18:50:33 18:50:33 1 of 1 START sql view model nyc_bikes.citi_trips_minutes ....................... [RUN] 18:50:37 1 of 1 OK created sql view model nyc_bikes.citi_trips_minutes .................. [CREATE VIEW (0 processed) in 4.53s] 18:50:37 18:50:37 Finished running 1 view model in 0 hours 0 minutes and 8.17 seconds (8.17s). 18:50:37 18:50:37 Completed successfully 18:50:37 18:50:37 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 We are always glad when we see the soothing words “Completed successfully”. If you go to BigQuery, our data warehouse, you will see a new view called citi_trips_minutes already created. By default, the name of the newly formed view or table in the data warehouse will be that of the model used to create it. Similar to the above, the below run command will also work. dbt run --select citi_trips_minutes But how do we view this newly created result. Unlike a table, BigQuery does not offer the PREVIEW button. However, there is a way… Query Click on the Query button, select In new tab and a new tab will form. Copy this SQL query onto the tab and run it to display the result of our citi_trips_minutes view. SELECT * FROM `dbt-project-437116.nyc_bikes.citi_trips_minutes` Our trip_duration_min column of interest is onto the far right of our view. We had hinted earlier that there is no limit to the complexity or number of models you can create in dbt. Below is a sort of more complex model (but not much) compared to the earlier one. This model rounds of the minutes to one decimal place and also removes the null columns, thus reducing number of rows from 58, 937, 715 to 53, 108, 721. The model is saved as citi_trips_round.sql. {{ config(materialized=&#39;view&#39;) }} WITH citi_trips_round AS ( SELECT *, ROUND(trip_duration_min, 1) AS trip_min_round FROM ( SELECT *, tripduration / 60 AS trip_duration_min FROM `dbt-project-437116.nyc_bikes.citibike_trips` ) WHERE tripduration IS NOT NULL ) SELECT * FROM citi_trips_round Our model is saved as the citi_trips_round view in BigQuery. Create a new query tab in BigQuery to view the results. Also use the query tab to count the number of rows and compare that with those from the citibike_trips table. It’s a difference of 5.8 million rows of null value eliminated. SELECT * FROM `dbt-project-437116.nyc_bikes.citi_trips_round`; SELECT COUNT(*) FROM `dbt-project-437116.nyc_bikes.citi_trips_round`; -- This is the count of the original citibike_trips table SELECT COUNT(*) FROM `dbt-project-437116.nyc_bikes.citibike_trips`; You may be wondering what the advantage that dbt provides. After all, can’t one just use the SQL Query tab in BigQuery just to do the transformations and save the table? Fine, that can work. However, dbt offers a form of persistence to your models. You see, an SQL query tab can be erased or modified, but the SQL queries in dbt even if they too can be modified at will they are persisted longer if they are saved in a versioning environment such as Github. One more thing, you don’t have to use the models/example folder. You can choose to rename this one, or save the two models we created in a different folder. They will still run fine. A view is a virtual table, similar to the original table, the physical dataset it was created from↩︎ "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
